% !TeX encoding = UTF-8
% !TeX program = pdflatex
% !TeX spellcheck = en_US

\documentclass[LaM,binding=0.6cm]{sapthesis}

\usepackage{microtype}

\usepackage{hyperref}
\hypersetup{pdftitle={Usage example of the Sapthesis class for a Laurea Magistrale thesis in English},pdfauthor={Francesco Biccari}}

% Remove in a normal thesis
\usepackage{lipsum}
\usepackage{curve2e}
\definecolor{gray}{gray}{0.4}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amssymb}
%\usepackage{algorithmicx}

\usepackage{algorithm}
\usepackage{todonotes}
\usepackage{float}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{algcompatible}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\DeclareCaptionFormat{myformat}{#3}
\captionsetup[algorithm]{format=myformat}
\DeclareMathOperator*{\argmax}{arg\,max}
% \algblockdefx{MRepeat}{EndRepeat}{\textbf{repeat}}{}
% \algnotext{EndRepeat}

\usepackage[noend]{algpseudocode}

\usepackage[acronym]{glossaries}

\usepackage[utf8]{inputenc} % this is needed for umlauts
\usepackage[T1]{fontenc}    % this is needed for correct output of umlauts in pdf

% \renewcommand{\thealgorithm}{\arabic{chapter}.\arabic{algorithm}} 


\newcommand{\bs}{\textbackslash}
\newcommand{\R}{\mathbb{R}}

% Commands for the titlepage
\title{A Large-scale Multi UAVs Reinforcement Learning Framework with Multiple Abstraction Layers}
\author{Giuseppe Capaldi}
\IDnumber{1699498}
\course{Engineering in Computer Science}
\courseorganizer{Facolt\`{a} di Ingegneria dell'informazione, informatica e statistica}
\AcademicYear{2019/2020}
\copyyear{2021}
\advisor{Prof. Luca Iocchi}
\advisor{}
\coadvisor{}
\authoremail{giuppocapaldi@gmail.com}

\examdate{15 January 2021} 

\examiner{Prof. Marco Schaerf }
\examiner{Prof. Silvia Bonomi}
\examiner{Prof. Giuseppe Antonio Di Luna}
\examiner{Prof. Pierangelo Di Sanzo}
\examiner{Prof. Luca Iocchi}
\examiner{Prof. Christian Napoli}
\examiner{Prof. Roberto Navigli}
\examiner{Prof. Simone Scardapane}

\versiondate{\today}
%====================================%====================================
%								MY PACKAGES
%%====================================%====================================
\usepackage{amsmath}
\graphicspath{ {./figures/} }

\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\fname@algorithm~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
  
%%====================================%====================================
%%   ACRONIMI
%%====================================%====================================


\makeglossaries

\newacronym{malrl}{MALRL}{Multiple Abstraction Layers Reinforcement Learning}
\newacronym{gl}{GL}{Grid Layer}
\newacronym{s3dl}{S3DL}{Simplified 3D Layer}
\newacronym{g3dl}{G3DL}{Georeferenced 3D Layer}
\newacronym{rwtl}{RWTL}{Real World Testing Layer}
\begin{document}

\frontmatter

\maketitle

\dedication{Dedicated to\\ my family}

\begin{abstract}
Unmanned aerial vehicles (UAVs) are rapidly spreading in many different fields, setting new challenges, risks and opportunities in several real-life applications from civilian (surveillance, industrial monitoring, agricultural services, distaster relief, SAR) to military services (air exploration, battlefield surveillance, target localization, target tracking, target locking)\cite{Shakhatreh_2019}.\\
In many specific applications, for example in infrastructural inspection and damage assessment, using a UAV instead of traditional tools has been proven to be more efficient. Parallel to the spread of use of single UAVs, many studies are focusing onto the usage of groups of UAVs, cooperating or even competing with each other, in order to achieve higher efficiency, performance and resilience\cite{sysArch11}\cite{sysArch12}.\\
The usage of multiple UAVs imposes the need of coordination and formation control when cooperating for the success of a mission.
In the future multiple UAVs will coexist in the same large urban space, conducting different operations as single vehicle or as fleets, probably without broadcast communication or cooperation among them, considering the presence of different actors and communication problems as delays or custom protocols to interface.\\
In this context extending the view to a large-scale perspective, both in terms of number of vehicles and of space dimensions could be beneficial especially to define and try to mitigate the risk of mid-air collision. \\
Artificial intelligence and in particular Reinforcement Learning techniques could help in the choice of policies able to reduce the collision risk, defining trajectories computed to be sufficiently distant one from the other without changing starting or final positions.\\
%TODO check se c è sta parte
In collaboration with Universitat Politecnica de Valencia as well as other companies and agencies, a collision model was developed from researchers at Sapienza University of Rome coordinated by prof. Luca Iocchi as part of BUBBLES project \cite{bubbles}. This model offers the opportunity to pass as input trajectories generated with the proposed framework and obtain as result risk indicators to be later compared for a risk reduction analysis.
\\
Virtual simulations can be used to find the best policies tzo be later tested and implemented in real life. 
Simulation is crucial for RL where several attempts (episodes) are required to train the agent, that would be too expensive to do with real agents, both in terms of time and economic costs.
\\
This master thesis will present methodologies, implementation and evaluations details of a multi-layer framework called "\acrshort{malrl}" (\acrlong{malrl}) based on multiple abstractions levels, aiming to transpose and improve existing Reinforcement Learning implementations to a 3D Realistic simulated environment.\\
In this sense the following project could be considered as a solid starting point towards further developments that could lead in the near future to the transposition of pure RL training, using abstracted environments, into real world UAVs missions.
 
 %The use of UAV will grow even more in the future and our cities will be impacted from this considerable change. 
\end{abstract}

\begin{acknowledgments}
Ho deciso di scrivere i ringraziamenti in italiano
per dimostrare la mia gratitudine verso tutti coloro che mi hanno sostenuto in questo percorso accademico e di crescita personale.
Vorrei iniziare ringraziando il Prof. Luca Iocchi per avermi guidato con professionalit\'{a} e costanza, e per avermi affidato direttive chiare e utili allo sviluppo di questo e altri progetti. 
Sento di dover ringraziare anche i miei colleghi, in particolare Alessandro Trapasso, per il lavoro svolto assieme e Federico Fiorini, per le consulenze rigurdo agli UAS.
Un ringraziamento va anche al Dott. Damiano Brunori che ha messo a disposizione mia e del team di lavoro la sua esperienza e conoscenza in una costante collaborazione.
Infine un ringraziamento speciale, va alla mia famiglia che non ha mai smesso di sostenermi, perciò è a loro che è rivolta la mia dedica.

\end{acknowledgments}

\tableofcontents


\listoffigures
\listoftables
\mainmatter

\chapter{Introduction}
\section{The Demand for Multi UAVs and Collision Avoidance}

The use of multi-UAVs for visual monitoring can hold several practical applications, most of them are based on a representation of the object under investigation as imagery data. Cameras are very often used on UAVs, due to low impact on the payload and possibilities associated with them, from the trivial pilot control using FPV (First Person View), with which the pilot can observe from the drone perspective in real time, to monitoring, target detection, localization, classification etc. \cite{mainSurvey}. \\
An imperative requirement for the integration of multi-UAVs in all applications is the capability to flight safely within their mission space. To do so collision avoidance in UAV system should be considered as an essential objective. The functionality of collision avoidance is to guarantee that collisions with other UAVs and/or targets can occur at a very low statistical probability value, obtained as a risk measure.\cite{pham}\\
Environment sensing should be considered as a first design challenge in order to obtain essential data such as size, speed, geo-location of other UAVs, targets, obstacles. Secondly decision making techniques on the base of the collected data should be enforced in an efficient, time-responsive manner.\\

\section{Motivation}
Noticing an incremental grow of interest in Unmanned Aircraft Systems (UAS), being a technology of recent adoption with many possibilities and risks for our future society but with engineering challenges as well, naturally arises the question on what can Artificial Intelligence do in this direction. If indeed autonomous driving undeniably seems the near future for car industry autonomous, UAVs too will be involved in this challenge, which will require precise safety guarantees. Among them collision avoidance seems one of the most interesting and challenging one.    
Along with this the personal interest in Reinforcement Learning algorithms and the discovery of AirSim tool were some of the elements that have been motivating this thesis.
Moreover what BUBBLES project, discussed in details later, tries to achieve with the fundamental requirement of technical expertise in different fields made a considerable contribution.
%Finally all the possible different characteristics of the situation such as type of UAV, model technical specification, power consumption, payloads, 2D or 3D trajectory paths, targets as moving or stationary objects should be taken into consideration.

\section{Framework Solution}
The need of collision avoidance for safe multi UAVs mission executions in the same area of space is considered as an essential requirement of \acrshort{malrl} framework, presented in this thesis. In this sense Reinforcement Learning techniques will make possible the generation of iterative 3D trajectories, representing different paths to reach the target. Collision avoidance is took into account after collecting the different trajectories (simulating air traffic in the area) that are generated iteratively, where the time component is ignored in the context of continuous or very frequent flights for each single route. Basically each generated trajectory will be affected by agent's initial position, desired final position (coincident with goal position and unknown to the agent), trajectories generated by other agents until that instant and obstacle positions. 

\subsection{Scenario Characteristics}
As it's been said UAVs may be involved inside a plethora of applications, leading to an equally large number of space scenario that needs to be restricted. For our purpose a specific urban city context has been considered that is the portion of a city or industrial area presenting a grid plane for its streets, that needs to be covered and buildings. Many cities, especially modern metropolis like New York, present a grid plane structure. Later it will be clear the importance of this assumption for our scenario in terms of possible UAVs trajectories.
\\
The specific application can be imagined as the covering of streets in a city while searching for a target (specific vehicle or suspect) of unknown position or in an industrial grid structure context to search for the presence of faults.

\subsection{Abstraction Layers}
The complexity of a real world scenario needs to be reduced introducing an abstracted design of all the most significant elements involved: environment with its obstacles and possible paths, targets and agents. This is true especially for urban areas where the complexity in terms of obstacles in the flight of UAV agents is increased. \\
Moving obstacles of any kind (especially at low altitudes) and human presence, to be protected, together with no-fly zones, restrictions, geo-fences makes urban spaces among the most challenging for multiple UAVs flights. \\
However these are the areas where multi UAVs applications can become more useful so this thesis aims to be a first step in this direction, abstracting some important aspects that will not nullify the validity of the work done, but that can be implemented customizing \acrshort{malrl} or introduced as future features. 

\subsection{Iterative Generation of Trajectories }
An uncommon approach has been used for the generation of UAVs trajectories where instead of simultaneous parallel simulated flights of multiple UAVs an offline iterative approach has been preferred. Here the generation of trajectories is incremental which means that in \acrshort{malrl} framework the flights are simulated one at a time and then sampled to produce fine-grained ordered points collections, each one representing a different trajectory. 
\\
This is done in two dimensional space so that the trajectories are then passed to the algorithm responsible for height assignment to each trajectory, moving the problem of collision avoidance into three-dimensional space.
\\
Therefore the asynchronous nature and offline nature of trajectory generation is conceptually detached from computational power limitations with respect to online simultaneous generation, leading the generation to possible large-scale dimensions.


\section{Advantages of Multiple Abstraction Layers}
\acrshort{malrl} framework it's composed of four abstraction layers. The first is an abstracted 2D grid plane representing the environment onto which reinforcement learning is applied and that lets generating an arbitrary number of trajectories in output. 
\\
Moving from 2D space towards 3D space, it was designed an algorithm responsible for height assignment for each trajectory, keeping the focus on collisions avoidance.
\\
The second layer is a 3D portion of space, built into Unreal Editor, delimited by fixed perimeter walls to restrict the area of simulation, it is a simplified replica of the city under study with blocks of the same size to simulate the presence of buildings. 
\\
The third layer is an extension of the precedent, it is still a 3D environment built into Unreal Editor, still delimited by perimeter walls but significantly larger and more importantly it is a faithful 1:1 replica of the city. 
\\
The fourth and last layer involves a real world deployment of the policies obtained and could be investigated in future works since the focus of this thesis was put onto the first three layers both for temporal and feasibility reasons.

\section{Objectives of Thesis}
The objectives characterizing this thesis are related to the design and development of a scalable and expandable framework, written in Python language, based on Gym toolkit and AirSim simulator, in order to use reinforcement learning algorithms for real world multi-UAVs applications, avoiding collision risks.\\
The scalability of the framework should be both in terms of number of UAVs considered and both in terms of space dimensions through the use of the already mentioned abstraction layers. 

\section{Results of Thesis}
To analyze some aspects of \acrshort{malrl} framework as the quality of solutions gained from QLearning and K-steps QLearning applications, the scalibility and compuatational time required from altitude assignment phase for vertical separation several experiments have been conducted.
In \ref{cres} it will possible to observe reports and evalution analysis on the results obtained from these experiments.  
\section{Organization of Thesis}
\todo[inline]{check consistency}

The rest of this thesis is organized as follows. \textbf{Chapter 2} reports related works, to give a theoretical perspective on prerequisites. \textbf{Chapter 3} provides the background in terms of software tools and implementation concepts. In \textbf{Chapter 4} the problems faced in each layer are formally defined. 
In \textbf{Chapter 5} architecture and design challenges related to the framework are discussed. \textbf{Chapter 6} contains details about implementation and development. In \textbf{Chapter 7} results from the training and conducted experiments are presented. \textbf{Chapter 8} concludes the thesis with potential ideas for future improvements.

%
%=====================================================================
%
\chapter{Related Work}
The majority of papers published on AirSim implement deep reinforcement learning algorithms, describe general characteristics of this relatively new platform and compare it against other possible solutions. Researches on using AirSim to achieve collision avoidance are easy to find but they majority focuses on dynamic collision avoidance, done in real time onboard by each drone.
\\
These techniques can enforce high level of safety but it's also affected by some limitations, one among the others is battery consumption. Offline static collision avoidance can be easier to implement and more efficient in contexts as BUBBLES plan to be present in the future, where missions will be arranged with Aeronautical authorities and follow specific regulations.
 % particularly important challenge in the field, since considers static collision avoidance can be considered as already available in many commercial products.
%\todo[inline]{FONTI}
However a systematic multi-layer approach exploiting specific geometric grid structure of cities to enforce 2D/3D trajectory generation was not found and seemed an interesting opportunity to explore the following themes.

\section{Reinforcement Learning}
Reinforcement Learning (Sutton et al., 1998) is an area of machine learning considered different both from \textit{supervised learning}, in which learning is obtained from a training set of labeled examples provided by a knowledgable external supervisor and both from \textit{unsupervised learning}, in which learning is obtained finding structures hidden in collections of unlabeled data. In this sense RL shares with unsupervised learning the absence of correct examples to rely on. However the goal of RL is to maximize a reward signal instead of trying to find hidden structures, something not directly addressed in unsupervised learning.\\
The basic idea of RL problems is capturing the most significant aspects of the real problem facing a learning agent interacting with its environment to achieve a goal. 
A learning agent will sense the state of its environment and take actions to affect it to achieve its defined goal(s). \\
RL consists of three primary elements: (i) the agent (learning agent acting on the environment); (ii) the environment; and (iii) the actions. To formally describe the environment Markov Decision Processes (MDPs) are introduced, indeed the problem of reinforcement learning can be formalized as the optimal control of incompletely-known MDPs.\\
A deterministic MDP can be defined as: 
$$MDP=< \textbf{S},\textbf{A},\delta,r > $$
where  \textbf{S} is a finite set of states; \textbf{A} is a finite set of possible actions; $\delta$ is a transition function ($\delta: \textbf{S} \times \textbf{A} \xrightarrow{}\textbf{S} $); \textit{r} is a reward function ($r: \textbf{S}\times\textbf{A} \rightarrow{} \R $).\\
MDPs must follow the Markov property so: $x_{t+1} = \delta(x_t,a_t)$ and $r_t = (x_t,a_t)$. In some cases the reward function ca be also defined as: $r: \textbf{S} \rightarrow{} \R$.



\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{RL}
	\caption{	Reinforcement learning general scheme from Sutton, R.S. and Barto, A.G., 2011. Reinforcement learning: An introduction.\cite{suttonRL}	}
	\label{fig: rl}
\end{figure}


\subsection{$Q$-Learning}
$Q$-learning \cite{watkins1992q} is one of the most famous algorithms studied in Reinforcement Learning and one of the first breaktrough in this field.\\
When the agent ignores the environment, Temporal Difference methods can be used to solve the MDP problem. The environment is said to be unknown for the agent when the transition probabilities and the reward expectation are unknown.
\\
$Q$-learning is an off-policy TD control algorithm that allows to learn the \emph{quality}, or expected utility, of each state-action combination. That is, for each state, all the expected rewards  obtained by taking each possible action at that particular state are estimated. This is represented as a state-action table called Q-table. A Q-table can be used to define a policy by always picking the action with the highest expected return.
\\
More formally, $Q$ function can be estimated as: $Q: S \times A \to \mathbb{R}$. $Q$ can be modeled as a mapping table (initialised with some uniform values), whose value is updated at each time step of our simulations.
\\Here's how our Q-table is updated at each time step $t$:
\begin{equation} \label{eq:$Q$-learningUpdate}
\begin{aligned}
  Q(s_{t},a_{t}) ={} & \underbrace{Q(s_t,a_t)}_{\rm old~value} +
  \underbrace{\alpha}_{\rm learning~rate} \times \\
   &\times \left[
    \overbrace{\underbrace{r_{t+1}}_{\rm reward} + \underbrace{\gamma}_{\rm
        discount~factor} \underbrace{\max_{a}Q(s_{t+1}, a)}_{\rm
        estimate~of~optimal~future~value}}^{\rm learned~value} -
    \underbrace{Q(s_t,a_t)}_{\rm old~value} \right]
\end{aligned}
\end{equation}

where:
\begin{itemize}
	\item $\alpha\in[0,1]$ is the \emph{learning rate}, a coefficient that regulates how much the newly learned values will contribute in the update
	\item $\gamma\in[0,1]$ is the \emph{discount factor}, a coefficient that controls the weight of future rewards. Values closer to 0 will make our agent "short-sighted", considering only the immediate rewards.
\end{itemize}
Then the algorithm is the following:\\

\begin{algorithm}[H]
\caption{\textbf{$Q$-learning}}
\label{alg:$Q$-learning}
\begin{algorithmic}[1]
\Require
 	\State $S$ is a set of states
 	\State $A$ is a set of actions
 	\State $\gamma$ the discount reward factor
 	\State $\alpha$ is the learning rate
 	\State $n$ is number of episodes to run $Q$-learning
 	\State $\epsilon$, probability to take random action, rather than follow policy
\Procedure{$Q$-learning} {}
\\Initialize $Q(s,a)$ will all 0 utility values.
\For{each episode $e_i$ with $i=0...n$}
\\ \qquad Initialize $s$
\For{each step of episode}
\\ \qquad \qquad Choose $a_t$ from $s_t$ using policy derived from $Q$ with $\epsilon$-Greedy
\\ \qquad \qquad Take action $a_t$, observe reward $r$ and $s_{t+1}$
\\ \qquad \qquad Update Q-table using equation~\ref{eq:$Q$-learningUpdate}
\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
Each episode converges to a terminal state or is stopped after a fixed number of steps.\\
The action choice in Step 12 uses the exploration/exploitation policy (function)
$EEP(s_t,Q_t,S,A)$ that is defined as follows:
\[   
EEP(s_t,Q_t,S,A)=a_t= 
\begin{cases}
\underset{a\in A}{arg \max}\ Q_t(s_t,a_t) &\quad\text{\textit{exploitation} with probability}\  1-\epsilon \\
\ & \\
\underset{a\in A}{rand(a)} &\quad\text{\textit{exploration} with probability}\  \epsilon
\end{cases}
\]
The above EEP function implements a policy denoted as the “greedy policy” where $\epsilon$ is often chosen as a small probability (i.e., 0.05).
 

\subsection{K-Step $Q$-learning}
K-steps version of $Q$-learning is a modified version of standard $Q$-learning that updates Q-table with the aforementioned equation, not only for current state-action pair but also for k pairs before terminal state. So the algorithm keeps memory of the last $k$ state-action pairs: $\{(s_1,a_1),...,(s_k,a_k)$\} s.t. $s_k$ is the second last state in the episode and the following state $s_{k+1}$ coincides with the terminal state.\\
So the new algorithm will result to be:

\begin{algorithm}[H]
\caption{\textbf{K-steps $Q$-learning}}
\label{alg:$K-steps Q$-learning}
\begin{algorithmic}[1]
\Require
 	\State $S$ is a set of states
 	\State $A$ is a set of actions
 	\State $\gamma$ the discount reward factor
 	\State $\alpha$ is the learning rate
 	\State $n$ is number of episodes to run $Q$-learning
 	\State $\epsilon$, probability to take random action, rather than follow policy
\Procedure{K-Steps $Q$-learning} {}
\\Initialize $Q(s,a)$ will all 0 utility values
\\Initialize $Queue$ as empty queue

\For{each episode $e_i$ with $i=0...n$}
\\ \qquad Initialize $s$
\For{each step of episode}
\\ \qquad \qquad Choose $a_t$ from $s_t$ using policy derived from $Q$ with $\epsilon$-Greedy 

\\ \qquad \qquad Take action $a_t$, observe reward $r$ and $s_{t+1}$
\\ \qquad \qquad Save $(s_{t+1},a_{t+1})$ in $Queue$ (\textbf{if} $\vert Queue \vert=k$, remove first element of $Queue$)
\EndFor
\For{ $(s_i,a_i),(s_{i+1},\cdot) \in Queue$ }  
\\
\qquad \qquad Update Q-table using equation~\ref{eq:$Q$-learningUpdate} where $s_t=s_i,a_t=a_i,s_{t+1}=s_{i+1}$
\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Hierarchical Reinforcement Learning and Hierarchical Abstract Machines}
In Andrey Kurenkov's essay \cite{kurenkov2018reinforcementflaw} some of the major limitations in RL field are discussed, they include:
\begin{itemize}

\item Sample efficiency: data generation is often a bottleneck and current RL methods are often data inefficient;

\item Scaling up: the application of classic RL to the problems with large action and/or state space is infeasible (curse of dimensionality);

\item Generalization: trained agents can solve complex tasks, but if we want them to transfer their experience to new (even similar) environments, most state of the art RL algorithms will fail (brittleness due to overspecialization);

\item Abstraction: state and temporal abstractions allow to simplify the problem since resulting sub-tasks can effectively be solved by RL approaches (better knowledge representation).

\end{itemize}
Major flaws in RL are relative to scaling issues, Hierarchical reinforcement learning (HRL) is a computational approach intended to address these issues by learning to operate on different levels of temporal abstraction.
\\
HRL methods learn a policy made up of multiple layers, each of which is responsible for control at a different level of temporal abstraction. Indeed HRL extends the set of available actions so that the agent can now choose to perform not only elementary actions, but also macro-actions, i.e. sequences of lower-level actions. Hence, with actions that are extended over time, we must take into account the time elapsed between decision-making moments. Luckily, MDP planning and learning algorithms can easily be extended to accommodate HRL.
\\
The work on hierarchical learners dates back to at least 1952, when Ashby developed a gating mechanism to handle recurrent situations and repetitive environmental disturbances.
The early work in hierarchical planning aimed at building a gating mechanism that switches between known behaviors. A subsequent system proposed a feudal representation [67], in which a hierarchy of managers and submanagers controlled an agent.
\\
A subsequent system proposed a so called "feudal" (inspired by Medieval Europe's Feudal system) representation where two principles holds:
the first is information hiding, since environment is observed at different resolutions, sub-managers only obey the sub-tasks they are set to without knowing whether it satisfies the higher level managers’ goal;
the second is reward hiding since communication is made between managers and "workers" through goals (a reward is given for reaching them).
A noteworthy effect of information and reward hiding is that the managers only need to know the state of the system at the granularity of their own choices of tasks. They also don't know what choices their workers have made to satisfy their command, since it is not needed for the system setup to learn, managers only know the states at the granularity of their own choices of tasks, sub-managers don’t know the task their manager is set to and super-managers don’t know the choices its manager made.
\\
A subsequent system proposed a feudal representation [67], in which a hierarchy of managers and submanagers controlled an agent. The aim of the feudal planner is not to pick a submanager, but instead to give an appropriate command to that submanager. In fact, each level has exactly one manager. All the managers learn the values of possible commands they can give to their submanager, given the current state and the command given to them by their own manager. At the lowest level, a command is just a primitive action.  \\
Unfortunately, the Feudal $Q$-learning algorithm is tailored to a specific kind of problem and does not converge to any well-defined optimal policy but it has paved the way for many other contributions.
% -------------------------- HAMS-----------------------------------
\\

An other representation for specifying a hierarchical structure is the hierarchy of abstract machines (HAM).
\begin{definition}[Hierarchy of Abstract Machines]
A hierarchical abstract machine (HAM) is
defined as a finite-state automaton with each automaton node marked with one of the following:
\begin{itemize}
\item Action: Execute a primitive action in the environment.
\item Call: Call another abstract machine as a subroutine.
\item Choice: Non-deterministically select the next machine node.
\item Stop/Return: Halt the execution of the machine and return to the previous call node, if any.
\end{itemize}
\end{definition}
An abstract machine is nothing but a specific representation of a partial policy, one in which either the action to be executed is known, or if in a choice node, the action must be decided among the possible ones. A hierarchical abstract machine has the added power to call other lower-level machines as subroutines — this leads to modular understanding and execution. \\
Another way to understand a HAM is that it is a partial algorithm/program that encodes our knowledge regarding various subproblems. For example, a simple way to traverse east could be go east, until there is an obstacle. If there is an obstacle, back off and go north or south a few steps, and when the obstacle is cleared go back to going east.\\
This is easily encoded as a HAM, by having machines for going east, north, and south. The high-level machine will be for going east, and when an obstacle is seen, a choice node, which selects between north and south machines, will come into play.\\
While options create more action choices by creating more complex actions, HAM can also constrain the set of allowed actions. Where task hierarchy can only specify a high-level goal (pseudoreward), HAM can also specify specific ground actions. Thus, HAM allows for a flexible way for the domain designer
to specify the domain-control knowledge. HAM is the most expressive among hierarchical structure representations.
\\
What is missing both in HAMs and HRL in general, is the explicit formulation of data computed in one layer and passed to the higher layer, something \acrshort{malrl} framework tries to enforce. \cite{BerliacHierachialRL2019,planMDP}



\section{Space-based Collision Avoidance Framework for Autonomous Vehicles}
In \cite{YU201837} a multi-agent collision avoidance framework is presented with the focus on design of general architecture, less on AirSim implementation.
\\
In the paper the importance of collision avoidance in autonomous systems and in particular in autonomous cars applications is addressed, mentioning well known cases of autonomous cars accidents. The context appear to be different from UAVs collision avoidance in which we have the third dimension (altitude) to consider,
offering new possibilities for avoidance but also different dynamics, new challenges and type of collisions.
\\
The paper focuses on properly capturing and  accounting  for  the  high  variability  of  geometries,  shapes,  and  sizes  of  the  agents (e.g.,  18  wheels  truck  vs.  4  doors  sedan),  something critical  in  situations  with  high  risk  of  accident  (e.g.,  intersection crossing). 
\\
Spatial-temporal   collision  avoidance  algorithms  are  defined, their goal is  to  ensure  the  accurate  prediction  of  the  collision  and  correct  decision  on  the  appropriate  steps  to  avoid  its  occurrence.  
Since implementation  and  simulation in AirSim of the proposed solution was  reported to be under development  relative to cars approaching a lightless intersection crossing, not much information was found about the argument.  
\\
This paper shares with the current thesis the accounting for the importance of collision avoidance in a near future of autonomous vehicles spreading and for the idea of designing and developing a complex framework able to exploit the dynamic models, collision detection capabilities, 3D realistic rendering and ease of virtual 3D world construction of AirSim.

\section{BUBBLES Project}
As student of Sapienza University of Rome, that is one of the participant together with Universidade de Coimbra, Eurocontrol (European Organisation for the Safety of Air Navigation) and Indra Sistemas SA at the European project BUBBLES,
coordinated by Universitat Politècnica de València, the project resulted to be very intersting and it guided towards choices and specific objectives for this thesis.
\\
The EU-funded BUBBLES project aims at defining separation minima and methods to UAS flying in the VLL SES (below 150 m) to improve the overall performance and safety guarantees. It will develop algorithms to compute the collision probability between Unmanned Aircraft Systems (UAS) using separation minima to keep it under acceptable levels. Moreover, the project itself will investigate how AI can contribute to dynamically managing these minima using different separation methods and agents (from centralised strategic deconfliction to distributed self-separation). BUBBLES will follow an operation centric, risk-based approach, assigning separation minima and methods depending on the UAS ConOps and the available U-Space services. BUBBLES will also extend the concept of Performance Based CNS to the UAS operations to draft safety and performance requirements.
\\
BUBBLES will develop Artificial Intelligence (AI) based algorithms to compute the collision risk of UAS leading to separation minima and methods so that a Target Level of Safety (TLS) stated in terms of overall probability of collision can be defined and maintained.
\\
These algorithms will be applied to a set of generic ConOps for UAS operations defined by BUBBLES, detailed enough to cover most of the envisaged applications, but generic enough not to be linked to any particular one. They will be classified in terms of risk using the SORA (Specific Operations Risk Assessment) methodology. SORA is a multi-stage process of risk assessment aiming at risk analysis of certain unmanned aircraft operations, as well as defining necessary mitigations and robustness levels.\\
These separation minima and methods will be assigned to the ConOps using AI techniques, leading to the definition of a set of generic OSEDs from which safety and performance requirements for the CNS systems will be derived applying a performance based approach.

\section{Vertical Separation} \label{verSep}
Vertical separation, as the name suggests, is the requirement to use a specific altimeter pressure setting within designated airspace and to operate at different levels expressed in terms of altitude or flight level.
\\
ICAO specify minimum vertical separation for IFR flight as 1000 ft (300 m) below FL290 and 2000 ft (600 m) above FL290, except where Reduced Vertical Separation Minima (RVSM) apply. Most national authorities follow a similar rule, but may specify a different level at which the rule changes.
\\
If, during an emergency situation, it is not possible to ensure that the applicable horizontal separation can be maintained, emergency separation of half the applicable vertical separation minimum may be used. This means that a 1000 ft vertical separation minimum may be reduced to 500 ft and 2000 ft vertical separation minimum may be reduced to 1000 ft.
\\
The use of emergency separation is described in ICAO Doc 4444, 15.7.1.
\cite{verSep}

\chapter{Background}
    In this chapter it will be explained the set of software tools and implementation concepts involved in the process of building \acrshort{malrl} framework.  
\section{OpenAI Gym}
Gym is a open-source toolkit for developing reinforcement learning algorithms in Python. It makes no assumptions about the structure of the agent and is compatible with any numerical computation library such as TensorFlow or Theano.
\\
The gym library is a collection of test problems — environments — that can be used to train and test any reinforcement learning algorithm. These environments have a shared interface, allowing to write general algorithms and to choose if using predefined environments, for well known problems as "CartPole" or build a new environment from scratch, as it has been done for \acrshort{gl} of \acrshort{malrl} framework.
\\
Gym usage is growing rapidly in RL Research field due to the possibilities it offers to mitigate two significant limitation for RL:
the need for better benchmarks and the Lack of standardization of environments used in publications.
\\
In supervised learning, progress has been driven by large labeled datasets of common use like ImageNet, CIFAR and others. In RL, the closest equivalent would be a large and diverse collection of environments. However, the existing open-source collections of RL environments don’t have enough variety and they are often difficult to even set up and use.
\\
Talking about different implementations of very similar environments, it's easy to notice subtle differences in the problem definition, such as the reward function or the set of actions, can drastically alter a task’s difficulty. This issue makes it difficult to reproduce published research and compare results from different papers.

\section{AirSim}
AirSim is a 3D simulator for drones, cars and more, built for Unreal Engine software (enabling also Unity editor but only in experimental releases). Airsim is open-source, cross platform, and supports \textit{software-in-the-loop} simulation with popular flight controllers such as PX4 and ArduPilot and \textit{hardware-in-loop} with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin and can be implemented into any Unreal environment (experimental support is present also for Unity, even if not tested in this thesis). 
\\
AirSim aims to be a platform for AI research to experiment with deep learning, computer vision and reinforcement learning algorithms for autonomous vehicles. For this purpose, AirSim exposes APIs to retrieve data and control vehicles in a platform independent way.
\\
Airsim is in continuous development and recently has added multi threaded vehicles simulation, opening new scenarios in the field of Multi Agent Reinforcement learning. 

\section{Blender}
Blender is a free and open source 3D creation suite. It supports the entirety of the 3D pipeline—modeling, rigging, animation, simulation, rendering, compositing and motion tracking, video editing and 2D animation pipeline.
\\
For the purpose of this thesis a very limited part of this software was used, exploiting the presence of a free plugin enabling 3D OSM data rendering. 

\section{GeoReferencing}
Georeferencing is usually defined as the process of transforming the internal coordinate system of a dataset to geographic coordinates as latitude, longitude and elevation (GPS). In the case of a picture (2D space), georeferencig can be described as the set of transformations needed to assign a tuple of latitude, longitude and optionally elevation information to a pixel or to a group of pixels. \\
Similarly georeferencing could be applied to a 3D space model transforming its internal coordinate systems into GPS coordinates.
\\
As everyone knows earth is an irregularly shaped ellipsoid and in cartography an ellipsoid coordinate system (Lon, Lat) is used to define positions, but very often a 2D visualization is desired where Cartesian coordinate system holds. To do so a projection coordinate system is used to project the ellipsoidal shape of earth onto a flat two-dimensional Cartesian coordinate plane.\\ However every projection has intrinsic distortion that may effect the area and the shape of real world objects it aims to represent. 
\\
To georeferenciate a virtual 3D space GPS accurate geographic data sources are needed to be later transformed (even scaled) to local coordinates in 3D space.
For this purpose it was used OpenStreetMap. 
\\
\todo[inline]{DETAILS ON COORDINATE Systems needed?}
%OSM DATA

\section{OpenStreetMap}
OpenStreetMap (OSM) is a free, editable map of the whole world that is being built by volunteers largely from scratch and released with an open-content license.
\\
The OpenStreetMap License allows free (or partially free) access to its map images but more importantly it allows access to all of its underlying map data. The project aims to promote new and interesting uses of this data. It is useless to explain the details involved in the use of open-source geographic data against propretary, coporighted software such as Google Maps, but it should be noted that OSM is not subject to any licensing fees or contractual restrictions.
\\
OSM map is accessible to everyone and in many countries OpenStreetMap is a used as an everyday viable alternative to other map providers. The main disadvantage of OSM is its not finished state, however the absence of data is limited to remote regions and the growing community is very active. \cite{geoRef}\cite{OSM}

\section{K-Dimensional Trees}
Finding all neighbors of a point inside a given sphere of fixed radius in 2D or 3D space is an integral part of many approaches using three-dimensional data. Choosing the most efficient data structure and algorithm to range query these points becomes crucial. Among the many possibilities range trees and kdtrees constitutes both valid data structures. It will be discussed the latter, since the choice fell on it and its advantages over the former.\\
K-D trees (also called as k-Dimensional trees) were invented in 1970s by Jon Bentley, they are binary search trees where data in each node is a K-Dimensional point in space. 
Since they are a space partitioning data structure for organizing points in a K-Dimensional space, they are often extended and implemented into 2D or 3D space simulations.
\\
Consider the following problem in 2D space (but it applies also for higher dimensions): given a large set of points \textit{S} compute the subset of \textit{S} of points within a given range distance from a given point \textit{p}. 
K-D trees allows avoiding a trivial "brute-force" approach, calculating the distance of \textit{p} from every point in \textit{S} through splitting the space into subspaces.
Every non-leaf node in K-D tree divides the space into two parts, called as half-spaces.
\\
Points to the left of this space are represented by the left subtree of that node and points to the right of the space are represented by the right subtree. 

%Each level has a “cutting dimension” • Cycle through the dimensions as you walk down the tree. • Each node contains a point P = (x,y) • To find (x’,y’) you only compare coordinate from the cutting dimension is x’ < x?

\todo[inline]{ pseudo code of algorihtm in KDtrees for query radius}

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{kdtree}
	\caption{
		A 3-dimensional k-d tree. The first split (red) cuts the root cell (white) into two subcells, each of which is then split (green) into two subcells. Finally, each of those four is split (blue) into two subcells. Since there is no more splitting, the final eight are called leaf cells.
	}
	\label{fig: kdtree}
\end{figure}

%
% PROBLEM
%
\chapter{Problems definition}
 This chapter will explore in details the main elements composing \acrshort{malrl} framework, called "abstraction layers" or simply "layers", from a theoretical and empirical problem definition perspective.\\
 To accurately define the problems involved in each layer we need to understand their characteristic structure and also their objectives and issues that distinguish them. We will use as a convention a different name for each layer: the first layer, which is the first one modelling and processing the input data, is called "\acrlong{gl}" (\acrshort{gl}) due to the grid plane structure of the environment; the second layer is called "\acrlong{s3dl}" (\acrshort{s3dl}) cause data obtained from previous layer are processed and included in this layer that makes use of a 3D environment (in Unreal Editor) that is a simplified reconstruction of the world in analysis; the third layer is called "\acrlong{g3dl}" (\acrshort{g3dl}) since the use of OpenStreetMap data to obtain a GPS georefererenced world (in Unreal Editor); the last layer is the "\acrlong{rwtl}" (\acrshort{rwtl}), that as the name suggests is relative to a (future) implementation of results obtained in the other layers in a real contest.
 \\
 The order in which abstraction layers are designed and presented (first l. to fourth) follows a top-down approach, from the general modeling of the environment with a top view on it considering only two dimensions up to a specific 3D realistic simulation or even the real world itself. Hence the concept of "abstraction", since each part of \acrshort{malrl} involves methods and methodology related to different level of abstractions of the real world, while the term "layers" highlight the systematic structure of this part in an homogeneous interconnected architecture. 
 
\section{\acrlong{gl}}

The Grid Layer is the most abstract layer and involves a 2D approximation of the city of choice. To conduct experiments using MALRL the location chosen to be modeled was a square shaped portion of Barcelona's city map. To model the 2D Environment as accurately as possible some measurement were made. The chosen district portion, situated in the historical center of Barcelona, presents a grid plane urban structure particularly symmetrical and this eased the general design of 2D and 3D maps. It should be noted that entire cities or districts of them present a similar grid plane structure for which the framework could be used.
\\
In the area delimited by chosen latitude and longitude bounds, streets and buildings were measured through satellite images from a top view perspective. Streets appeared to have a width of approximately 20 meters whereas almost all buildings in the chosen portion occupy a surface equal to 120 square meters . 
\\
\acrshort{gl} uses a 2D grid space composed of square blocks where each square has a side representing 20 meters in real world, so a single line of blocks is enough to model a street whereas buildings are represented as $12 \times 12$ blocks. Buildings and streets are the only map elements represented in this abstraction layers but other elements (e.g dead-ends, no-fly zones,...)  are easily implementable. The use of precise measures for the 2D model is out of the purpose of this thesis, also considering the possibility to reduce block scale in order to represent more precisely the real world. However in that case a trade-off between 2D space scale granularity and computational complexity need to be taken into account. \\
%TODO calcs of this complexity increase

\subsection{Problem in \acrlong{gl}} \label{pl1}
Starting from the first layer, what is required as input to this layer is the number of trajectories we want to generate, the width ($w_{space}$) and the height ($h_{space}$) in meters of the rectangle chosen as the portion of city to be represented, $w_{cell}$ and $h_{cell}$ as dimensions of a single board block (or cell), height will be considered in the next layers, since we are now in a bi-dimensional space.
\\
To ease the experiments it was chosen a value of twenty meters both for $h_{cell}$ and $w_{cell}$.
\\
Maze problems are a well known and often used test-bed to employ reinforcement learning algorithms. They are derived from gridworld problems, one of the first problem to explore while studying Reinforcement Learning.
%The presented framework allows to customize maze dimensions and composing elements with the aim of letting the future user analyze different conditions relative to the mission of UAVs. \\
The problem to be defined was considered to be well suited for a gridworld environment: a fixed number $\lceil \frac{w_{space}}{w_{cell}} \rceil \times 
\lceil \frac{h_{space}}{h_{cell}}\rceil $ of cells compose the board, each element (agent, obstacle portion or goal) occupy exactly one cell. As already specified to obtain greater precision while modeling the environment, the map scale could be decreased, in this case a single agent could require more than one block. In our experiments scale is fixed to be as one cell per element.\\
The state of the agent at time-step $t$ is composed of row and col numbers of the cell it is occupying:
$$s_t=(r,c),$$
where $r \in [0,width-1]$, $width = \lceil \frac{w_{space}}{w_{cell}} \rceil$, $c \in [0,height-1]$,$height = \lceil \frac{h_{space}}{h_{cell}}\rceil$  and $t \in [0,numSteps]$.      
\\
The agent starts from a fixed state $s_0$ and can move inside the board to a new state in the following directions: up, down, left and right. Every trajectory to be generated has its specific $s_0$, but they all share the same goal positions. \\
At each time step $t$ the agent can remain at its state (cell) or move to a new one doing an action:
$$ a_t \in [0,3],$$
where each different value of $a_t$ is equivalent to a direction. \\
The goal is defined as $g \in G$, $G$ is the set of goals to be collected in order to consider an episode concluded, i.e. when $\exists \ t\  s.t.\  s_t=g$. G can have unitary cardinality or greater than one, in that case an episode is concluded when all goals are collected i.e. when $\exists\  t\ s.t.\  s_t=g\  \forall g \in G$. From now on it will be considered the case when G has unitary cardinality, i.e. $\exists \ g$ a single fixed goal per each episode.
\\
The problem of avoiding previously generated paths is addressed through the memorization of previously generated trajectories, keeping in memory a data structure for cells already visited, i.e. $visitedCells$. Then the reward function will discourage policies in which the agent moves over an already visited cell.   
%The cells at $g$ position are cells in which UAV is supposed to consider completed its mission so eventually the agent will position itself over this cell, when this happen the episode is considered completed.
The reward function computes a reward for each step $t$ depending on the action done and the state in which the agent is:
\[   
r_t(s_t,a_t) = 
\begin{cases}
1000 &\quad\text{if }  s_t = g \quad\text{(agent reached the goal)}\\
-0.1 &\quad\text{if }  s_t = s_{t-1} \quad\text{(agent did not move)}\\
-5 &\quad\text{if }  s_t \in visitedCells \quad\text{(agent moved over previous paths)}\\
-0.1/(width\cdot {height}) &\quad\text{otherwise} \quad\text{(agent moved)}
\end{cases}
\]
\\
What is obtained when reinforcement learning is terminated is a set of trajectories: $$T^{l1} = \{T_{0,n_0}^{l1},...,T_{m,n_m}^{l1}\}$$ where $T_{i,n_i}^{l1}= \{s_{0},s_{1},...,s_{n_i}|s_{t} <_t s_{t+1} \}$ and $s_t$ has been already defined. 

\section{\acrlong{s3dl}}
\subsection{Problem in \acrlong{s3dl}}
In the previous layer every transaction was generated penalizing eventual overlapping trajectories in the xy plan, obtaining $T^{l1}$. However this is not sufficient to guarantee complete collision free trajectories even if increasing \textit{width} and \textit{height}, increasing the number of cells for the width of the street, distancing initial position of agents are all factors that can help in this sense. This is true especially for environments in which streets can hold a few number of paths or even just one path per street (like in our experiments).
\todo[inline]{Insert image relative to this}
Since $\forall \ T_i \in T^{l1}$, $T_i$ is obviously missing the coordinate for z axis, not only to decrease the risk of collision but also for the need of displaying the trajectory and let the agents follow it (this is done in \acrlong{s3dl} and three).\\
In this layer the third dimension was exploited for enabling the assignment of an altitude value to each trajectory, depending on possible collisions. Now collision detection is no more based on checking if two trajectories (set of cells) overlap but on checking instead if the two trajectories (now points in the Cartesian 2d space) have an euclidean distance below a certain threshold.
\\
The algorithm provides parameters to set: the \textit{radius}, equals to the threshold under which the euclidean distance $d(s_t,s_t')$, $\forall\ s_t \in T_1, s_t' \in T_2$, where $T_1,T_2 \in T^{l1}$, is considered as a (potential) collision between $T_1^{l1}$ and $T_2^{l1}$; the \textit{minimum} and \textit{maximum altitude} values defined as boundaries out of which flights are not permitted; the \textit{separation} value meaning the constant distance along z axis used to separate any two colliding trajectories.
\\

The following will be a formal description of the problem:
\\
we have a box B with dimensions $w_{s} \times h_{s} \times (M_{a}-m_{a})$,
where $w_{s}\equiv w_{space}$ was already defined as well as $h_s \equiv h_{space}$, while $m_{a}$ and $M_{a}$ are respectively minimum and maximum altitude (z coordinate value in Airsim in meters).
\\
No trajectory is allowed to be generated out of block B space. The origin of the axis is considered to be located in one of the vertices at the base of the block. In this layer the \textit{i-th} flight trajectory composed of n points (2D cells coordinates) in input is: 
$$T_{i,n}^{l1}=\{s_{0},s_1,...,s_{n}|s_{t}<_t s_{t+1} \}$$
where $a<_t b$ means $a$ was generated at timestep $t_a<t_b$, \\$s \in \R^2$, $ i = 0,1,...,\vert T^{l1}\vert $, $T^{l1}=\{ T_{0,n_0}^{l1},...,T_{m,n_m}^{l1} \}$.
\\
Given a value $r \in \R$ (\textit{radius}), the algorithm should transform each $T_{i}^{l1} \in \R^2$ into $T_{i}^{l2} \in \R^3 $, assigning a different altitude value $z \in [m_a,M_a]$ to each single point $s_t \in T_i^{l1}$, obtaining:
$$ T_{i,n}^{l2}=\{(s_{0,i},z_{0,i}),...,(s_{n,i},z_{n,i}) | (s_{t,i},z_{t,i})<_t (s_{t+1,i},z_{t+1,i})\}$$ 
and 
$$ T^{l2}=\{ T_{0,n_0}^{l2},...,T_{m,n_m}^{l2}  \}$$
\\
%Already said
%The $r$ value (\textit{radius}) introduced before is used by the algorithm since a potential collision between two flights e.g. flight $f1$ and flight $f2$ is considered to happen in case we have at least two points $s_{f1} = (x_{f1},y_{f1})$ and $s_{f2}=(x_{f2},y_{f2})$ in the same cartesian xy plan, so at same height where $d(x_{f1},y_{f1},x_{f2},y_{f2}) < r$, with $d(x_{f1},y_{f1},x_{f2},y_{f2}) = \sqrt {\left( {x_{f1} - x_{f2} } \right)^2 + \left( {y_{f1} - y_{f2} } \right)^2 }$



\section{\acrlong{g3dl}}
\subsection{Problem in \acrlong{g3dl}}
This layer enables the possibility to correct trajectories that are affected by the presence of new elements or by the absence of past elements in the environment with respect to the previous abstraction layer. The presence of static obstacles not represented in \acrlong{s3dl} but present in \acrlong{g3dl} can determine an invalid trajectory resulting in a collision in Airsim. On the contrary the absence of obstacles considered in \acrlong{s3dl} can determine very inefficient trajectories.
\\
These limitations can be formulated into the following problem:\\
UAV agent is following a given trajectory in 3D given by previous layer in AirSim. When a collision with any obstacle happens, the trajectory needs to be corrected in order to avoid the obstacle in future runs. So as soon as the collision event is triggered, the position of UAV in the space $s_t = (lat,lon,z)$ is saved in memory as a tuple composed of latitude, longitude and altitude (considered as absolute altitude, where ground level is z of zero value) coordinates. Then two constants are chosen: $N$,$M \in \mathbb{N}$ representing distances as number of time steps before and after current $t$ s.t. $\exists$ $s_{t-N}$,$s_{t+M} \in T_i^{l2}\in T^{l2}$ waypoints, otherwise $s_0$ and/or $s_{n_i}$ are taken instead. Once identified $s_{t-N}$,$s_{t+M}$, or their substitutes, they are saved and used respectively as initial and final positions for a new RL algorithm considered as a "local" problem solver.
\\
Indeed the problem of trajectory correction is localized to the problematic part of trajectories, replacing waypoints that resulted to be problematic causing a collision, with new waypoints generated by testing the policy obtained through local reinforcement learning.
\\
The values of $N$ and $M$ variables is increased in case of persistent collision after learning phase, to let the agent explore from more distant in time initial positions or final positions.\\
In this layer the set of previously generated trajectories in 3D $T^{l2}$ is modified into a new set of corrected trajectories $T^{l3}$ ready to be passed to the next layer. 
\\
% this goes in solution...
%To solve this issues a "patch system" based on RL has been designed. When a collision occur, the framework stop the simulation and save the position.

%\section{Problem in \acrlong{s3dl} and 3 }
%TODO remove this part
%Both in layers 2 and 3 we can define the problem to be solved as the following: in a 3D continuous space the agent is a multirotor vehicle, the dynamic of which is simulated using AirSim. When in "\textit{crab-mode}",the mode that was used to conduct the experiments, the agent can move at a fixed height along a single axis among x and y, keeping the vehicle front always pointing in the direction of travel. In this mode UAV can reproduce trajectories obtained in \textit{\acrshort{gl}}, keeping a consistent mode that limit the capability of movement of a UAV but can be considered useful when UAVs paths are constrained by high buildings or the mission requires paths following streets network.\\ In both layers previous trajectories are displayed as segments in 3D space connecting the points extracted from the previous phase of 2D reinforcement learning. After displaying these trajectories, points are stored as K-D trees in order to ease the computation when analyzing possible collision with the agent.
%TODO parla dell'interpolazione

%So given a set of 2D trajectories (ordered set of vectors of dimension 2), a safety distance measure (in meters), a minimum and maximum accepted height, the goal is to generate a new set of 3D trajectories which keeps unchanged x,y coordinates for each point but have z coordinate computed to be the minimum distance equal to the safety measure in input.\\

% TODO sposta sotto in architecture 
%Now the agent starts from a fixed position specified with local coordinates of Unreal Editor or with GPS coordinates (inside \textit{\acrlong{g3dl}}). Then it takes off moving along z axis until it reaches the height of first point and continues its navigation following the trajectory obtained from the previous phase. \\
%When the agent collides with any mesh in the 3D environment the environment is reset and the agent obtain a penalty. The same applyes when the agent collides with one or more points beloging to the other trajectories. 

\section{\acrlong{rwtl}} \label{pl4}
\subsection{Problem in \acrlong{rwtl}}
\todo[inline]{How to formalize the problem for real world?}

\chapter{MALRL Framework}

Inspired by the related works reported in Chapter 2 and considering the endless possibilities offered by the ever-growing research in the field of Reinforcement Learning and in particular in solutions to multi-agents path finding problems, the desire to explore those solutions in a 2D environment has taken hold.
\\
Moreover the simplicity to build complex and highly customizable 3D environments in Unreal Engine together with AirSim simplicity and accuracy in simulating UAVs missions, guided the work in the direction of a framework able to merge these two different levels of abstraction.
\\
Another opportunity has been offered by the possibility to take advantage of cities with a grid plan structure, suitable for a 2D grid block maze rapresentation.
Combining this last point with the presence of downloadable open geographic data, including 3D buildings and streets, the whole thing moved in the view of a more and more realistic testbed.
\\
In light of these consideration a framework composed of multiple interconnected abstraction layers, each one characterized by its own problem definition, approximation constraints and solutions, seemed to be an interesting and challenging design.
\\
In the following section an in depth explanation on the structure of \acrshort{malrl} framework as a whole is presented.

\section{Architecture}
\acrshort{malrl} framework is composed of exactly four layers having the first one as a discrete two dimensional gridworld while the remaining three as continuous three dimensional environments.  

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{Architecture.jpg}
	\caption{
		Architectural scheme of \acrshort{malrl} framework, showing the four layers of  abstraction.
	}
	\label{fig: \acrshort{malrl}architecture}
\end{figure}


\todo[inline]{check for repetitions wrt \acrlong{rwtl} }
In the figure \ref{fig: malrl architecture} is presented the general architecture of \acrshort{malrl} framework as a block diagram.\\
Starting from first layer they will described in details one by one, in the order of framework utilization.\\

\textbf{\acrlong{gl}} is the most abstract layer and involves a 2D approximation of the city of choice. To conduct experiments using this framework the location chosen to be modeled was a square shaped portion of Barcelona's city map. To model the 2D Environment as accurate as possibile some measurement were made. The chosen district portion, situated in the historical center of Barcelona, presents a grid plane urban structure particularly symmetrical and this eased the general design of 2D and 3D maps. It should be noted that entire cities or districts of them present a similar grid plane structure for which the framework could be used.
\\
In the area delimited by chosen latitude and longitude bounds, streets and buildings were measured through satellite images from a top view perspective. Streets appeared to have a width of approximately 20 meters whereas almost all buildings in the chosen portion occupy a surface equal to 120 square meters . 
\\
\acrshort{gl} uses a 2D grid space composed of square blocks where each square has a side representing 20 meters in real world, so a single line of blocks is enough to model a street whereas buildings are represented as $12 \times 12$ blocks. Buildings and streets are the only map elements represented in this abstraction layers but other elements (e.g dead-ends, no-fly zones,...)  are easily implementable. The use of precise measures for the 2D model is out of the purpose of this thesis, also considering the possibility to reduce block scale in order to represent more precisely the real world. However in that case a trade-off between 2D space scale granularity and computational complexity need to be taken into account. \\
%TODO calcs of this complexity increase

\textbf{\acrlong{s3dl}} involves the use of an offline algorithm designed from scratch able to move the generated 2D paths to 3d trajectories, where each waypoint has a specific $z$ coordinate in the range of possible ones.\\
The right choice of z is crucial to prevent mid-air collision, in this sense the algorithm focuses more on this aspect than on time-space optimization. \\

\todo[inline]{write pseudocode of altitude algo}

\textbf{\acrlong{g3dl}} makes use of 3D trajectories in input from above layer (\acrlong{s3dl}), however inserting new realistic elements, with their complex shapes, the different height of buildings and the deviation of measures due to georeferencing w.r.t. approximated measures of the previous layer. This gives the opportunity to use again RL techniques but as localized "patches", correcting only the parts of the trajectories that are problematic.
\\
 
\textbf{\acrlong{rwtl}} is just conceptually defined and it was not possibile to be implemented and tested in this work but it's considered as a possible future feature to explore w.r.t. its feasibility, limitations and promises. It is worth to mention in this sense one of the advantages in the use of AirSim tool that is the possibility to deploy learned policies directly in real world scenarios. Indeed sim-to-real deployment of models learned purely in virtual simulation, is something already achieved in \cite{bonatti2020learning}, where researchers managed to get over one kilometer of cumulative autonomous flight through obstacles in the real world.

\section{Collision Avoidance}
As specified collision avoidance was one of the objective of this thesis and required considerable effort while designing and developing \acrshort{malrl}, it is designed for \acrlong{s3dl} but its results are used also for the next layers. 
Several versions of the algorithm were created to accommodate possible needs that could depend on the characteristics of the application scenario such as from the possibility to change altitude during the mission to avoid other UAVs or be constrained at a constant altitude similar to what vertical separation (\ref{verSep}) does in civil aeronautics.\\
It could be identified two different types of collisions: \\
collisions with dynamic, moving objects, in this case other UAVs rapresented as trajectories and collisions with static obstacles in this case composed of 3D Buildings and ground. 
\\
As for increasing the granularity and general precision of 2D map modeling the city in \acrshort{gl} also the introduction of realistic elements in the map at \acrshort{gl} or two and three could improve the general results.
For example considering the possibility to introduce other moving or static objects as obstacles, elements like eletric wires of powerlines, streetlights, neon signs, no fly zones, geofences, here the possibilities are endless. 
\\For the purpose of this thesis static objects are considered avoided in \acrshort{gl} after having mapped them inside the the aforementioned grid plane (each state has a set of acceptable actions associated with, crossing a wall is not permitted). Potential mid-air collisions between UAVs instead should be considered also in \acrlong{s3dl}, since reinforcement learning should have discouraged overlapping paths but the absence of an optimality demonstration, in some cases impossible to reach due to the proximity of initial positions or goals, does not offer guarantees in these sense. 
\\
Regarding the collisions with dynamic objects, the idea is to assign altitude to waypoints of UAVs trajectories such that already explored paths are avoided inside the city, represented by the trajectories generated in \acrshort{gl}, this is done storing them as kd trees, details about this structure are specified in the next section. 


\section{Altitude Assignment Algorithms}
The assignment of the correct altitude to each UAV mission plan, until now simplified in the concept of trajectory as oredered set of points, is considered crucial to decrease the risk of collision. For the purpose of this thesis and also in the context of the BUBBLES project some limited tolerance is granted for proximity of trajectory points, considering the implementation of other layers of safety and algorithms in charge of enforcing the minimum possible risk in the future.
A pseudocode of the first designed algorithm is presented below. 


\begin{algorithm}[H]
\caption{\textbf{Altitude assignment for empty space}}
\label{alg:altAss1}
\begin{algorithmic}[1]
\Require
 	\State $T^{l1}$ is the set of $n$ trajectories from \acrshort{gl}
 	\State $o_a$ is the vertical separation value (offset in altitude)   
 	\State $m_a$ is the minimum possible altitude
 	\State $M_a$ is the maximum possible altitude
 	\State $m_{sp}$ is minimum number of safe-points
 	\State $r$ is the radius to find how many points from other trajectories   
\Procedure{AltitudeSchedulerV1}{}
\State trees $\gets \text{Build array of kd trees from } T^{l1}$
\State drones $\gets$ Get index i for each $T_i \in T^{l1}$
\State $T^{l2} \gets$ Init empty set of trajectories $ T_0^{l2},...,T_n^{l2} $
\State    collidingTrajs $ \gets$ Init empty dictionary

\For{$T_i$ in drones}
\For{point in $T_i$}
\State nSafePoints $\gets$ 0
\State nCollisions $\gets$ 0
\For{tree in trees}
\If{tree belongs to $T_i$} \Comment{It's the tree of current trajectory}
\State go to next tree
\EndIf
\State nCollisions $\gets$ Compute number of collision in radius $r$ from point using tree
\If{nCollisions > 0}  
\State                  collidingTrajs[i].append(indexOf(tree)) if not present
\EndIf
\EndFor
\If{nCollisions = 0}
\State            nSafePoints $\gets$ nSafePoints$\gets$ nSafePoints$+ 1$
\EndIf
\If{nSafePoints $\ge m_{sp}$ }
\State            collidingTrajs[i] $\gets$ $\varnothing$
\EndIf
\If{ i not in collidingTrajs or collidingTrajs[i]=$\varnothing$}
\State            newAltitude $\gets M_a$
\Else
\State            priorities $\gets$ i $\bigcup \ $ collidingTrajs[i]
\State            priorities.sort()
\State            offset $\gets$ priorities.index(i)
\State            newAltitude $\gets M_a - offset \cdot o_a $
\If{newAltitude < $m_a$}
\State
\Return $\varnothing$               
\Comment{Impossible to allocate due to $[m_a,M_a]$ limits}
\EndIf
\EndIf
\State point.z $\gets$ newAltitude
\State      $T_i^{l2}$.append(point)
\EndFor
\EndFor
\State
\Return $T^{l2}$

\EndProcedure
\end{algorithmic}
\end{algorithm}

This algorithm was designed for an empty space situation which consist in a case in which no UAVs have already be assigned to a trajectory, so no trajectory $T_i$ in input has already an associated altitude. The basic idea behind the algorithm is to give a priority to trajectories at the start position of the array in input, giving them the highest possible altitude but decreasing it of a multiple of $o_a$ when a trajectory with enough number of colliding points is found, and has a greater priority than the current.
\\
Later a second version of this algorithm was introduced in order to add as a feature the possibility to assign a correct altitude to trajectories of \acrshort{gl} ($T^{l1}$) given the presence of trajectories that cannot be moved since their altitude value has already been assigned ($T_{assigned}^{l2}$). In this sense the new space from empty is now called in general "busy". Together with the introduction of this feature new aspects have been taken into account leading to the introduction of new parameters.


\begin{algorithm}[H]
\caption{\textbf{Altitude assignment for busy space}}
\label{alg:altAss2}
\begin{algorithmic}[1]
\Require
 	\State $T^{l1}$ is the set of $n$ trajectories from \acrshort{gl}
  	\State  $T_{assigned}^{l2}$ is the set of pre-existing 3D trajectories
  	\State $M_{points}$ is the maximum number of allowed colliding points before considering a trajectory colliding
 	\State $o_a$ is the vertical separation value (offset in altitude)
 	\State $m_a$ is the minimum possible altitude
 	\State $M_a$ is the maximum possible altitude
 	\State $m_{sp}$ is minimum number of safe-points
 	\State $r$ is the radius to find how many points from other trajectories   
\Procedure{AltitudeSchedulerV2}{}


\For{ $T_i \in T_{assigned}^{l2}$ }
\State  k $\gets$ getAltitudeOf($T_i$)
\State v = buildKDTree($T_i$) \Comment{Create 2D Tree with points from $T_i$}
\State staticTrees.insert(k,v)
\EndFor
    

\For{$T_i \in T^{l1}$} 
\State  k $\gets M_a$ 
\State v = buildKDTree($T_i$)
\State mobileTrees.insert(k,v)
\EndFor
      
\State    proposedAltitude $\gets$ Init empty dictionary
\For{ $T_i \in T^{l1}  $}
\State        proposedAltitude[i]  $\gets M_a$
\EndFor
\algstore{myalg}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\begin{algorithmic}[1]
\algrestore{myalg}
\For{$ T_i \in T^{l1} $}
\State        assigned $\gets$ False
\While{assigned $\neq$ True}
\For{$ point \in T_i$}
\If{$proposedAltitude[i] \in staticTrees.keys()$}
\For{ tree in staticTrees[proposedAltitude[i]]}
\State nCollisions $\gets$ nCollisions+tree.findCollisionsFrom(point,$r$)
\If{ nCollisions > 0 }
\State                            nProblematic $\gets$ nProblematic + 1
\State \textbf{break}
\EndIf
\EndFor
\EndIf

\If{proposedAltitude[i] in mobileTrees.keys()}
\For{ tree in mobileTrees[proposedAltitude[i]]}
\State \textbf{continue} 
\If {$i$ = indexOf(tree)}
\Comment{Tree associated with $T_i$}
\EndIf
\State nCollisions $\gets$ nCollisions+tree.findCollisionsFrom(point,$r$)
\If{ nCollisions > 0}
\State nProblematic $\gets$ nProblematic + 1
\State \textbf{break}
\EndIf \EndFor \EndIf
\If{nProblematic $\ge M_{points}$}   
\State mobileTrees[proposedAltitude[i]].remove(getTreeByIndex(mobileTrees,i)) 
\State proposedAltitude[i] $\gets$  proposedAltitude[i] - $o_a$
\If{proposedAltitude[i] < $m_a$} 
\State
\Return $\varnothing$
\EndIf
\State                    mobileTrees.insert(proposedAltitude[i],getTreeByIndex(mobileTrees,i))
\State \textbf{break}
\EndIf
\EndFor

\If{nProblematic < $M_{points}$}
\State assigned $\gets$ True
\EndIf
\EndWhile
\EndFor

\State    $T^{l2} \gets$  $\varnothing$
\For{ $i \in $ proposedAltitude.keys()}
\State newAltitude $\gets$ proposedAltitude[i]
$newT_i$ = $\varnothing$
\For{ point in $T_i$ }
\State point.z $\gets$ newAltitude
\State $newT_i$.append(point) 
\EndFor
\State $T^{l2}$.append($newT_i$)
\EndFor
\Return $T^{l2}$

\EndProcedure
\end{algorithmic}
\end{algorithm}

\todo[inline]{find algo for this part below}
The algorithm can work in three different modes depending on managing colliding trajectory. A first mode is designed to assign a single height value per trajectory avoiding descents, assigning the same height value (equal to the maximum height) and decreasing it of a separation value each time a collision on the xy plane it's found. A second mode allows descents and let a trajectory be kept at the assigned height (maximum at the beginning, decreased of a separation value after a collision) until a new collision on the respective xy plan is found. A third mode tries to keep the trajectory at maximum height and decrease it only for the colliding points and for the next \textit{n} (parameter set by the user) following points. \\


\section{Scalability}
Scalability is also among the goals of this work and it is intended to be both in terms of modeled uavs, so trajectories, and both in terms of dimensions of the space. Regarding \acrshort{gl}, dimensions are easily expandable considering the possibility to change the scale associated to each 2D block composing the grid or to simply change map dimensions as number of rendered blocks.
\\ 
\acrlong{s3dl} has been designed to be smaller and simpler than the next Layer in order to ease computation and the work of the 3D user that is modeling the environment directly inside Unreal Engine editor. However this layer as also the next one (\acrlong{g3dl}) are both heavily extensible considering that graphics details could be adjusted according to the hardware equipment.
%TODO riconnettiti alla parte dei results


\chapter{Implementation Details}

\section{General Usage}
Each layer (except for \acrlong{rwtl}, cf \ref{pl4}) has a specific homonym Python script associated with it, ready to be launched. Scripts must be executed one by one following the numerical order so starting from the first layer then second, third and fourth. Any other modality of execution is not currently supported.
\\
The passage of data is done using csv files to store and then retrieve generated trajectories $T^{l1},T^{l2},T^{l3}$. This let analyze them easily, as it's been done to pass trajectories to the external risk model.  

\section{2D Reinforcement Learning}
As specified in the previous chapter, \acrshort{gl}'s problem formulated as a Reinforcement Learning problem. In particular $Q$-learning algorithm has been implemented from scratch and used in its K-steps version. The maze instead follows OpenAI Gym format so it's compatible with other environments sharing a similar structure (same action space and state space) and with other RL algorithms that follows O. Gym APIs.\\
The \textit{\acrshort{gl}} of the Framework is composed of a maze environment customizable and compatible with \textit{OpenAI Gym} toolkit enabling the possibility to apply other algorithms and test them against $Q$-learning.\\

\section{Maze Environment}
Our maze environment in \acrshort{gl} is inspired by \cite{mazeGit} where a simple classic maze with random walls is implemented. The clean and easily extendable code offered the possibility to use it as a starting point in coding our custom 2D environment representing city grid plane. Some elements have been exploited as the bitmap array used to encode all possible actions inside the grid plane. In this way escaping from boundaries or entering the cell of an obstacles cannot be chosen as possible actions. 
\\
Some elements of the original maze where not used while others have been built from scratch. For example original maze contained portals, cells in which the agent can enter to be teleported into another cell, without taking actions needed to reach that cell. 
\\Talking about the main new elements introduced in the code, we have: the introduction of multiple goals mode, where each of them need to be collected to complete an episode; the covering mode, in which the agent completes the episode only when all free cells of the board (the ones that do not represent neither obstacles neither goals or starting points) have been visited; the history of precedent actions, to save previous trajectories in order to discourage the generation of overlapping paths.
\\
These new elements and the general new design of the maze as a symmetrical grid plane city with square group of $6\times 6$ blocks were accompanied by some improvements in the part of the code related to the rendering of maze, disabled by default but it can be enabled through the relative CLI parameter. 



\section{AirSimGeo}
In order to build a geo-referenced world the following steps were used:

first the area of interest (min-max latitute,min-max longitude) was identified from the map available at OpenStreetMap web-site and downloaded as OSM file.
Then opening Blender open source 3D editor it was installed and used \textit{blender-osm}. It is an open-source plugin for Blender, easy to install and ready to be used that offers the possibility to download, load, manipulate OSM maps in order to transform them into 3D maps and texture overlay.
So from the downloaded OSM file after some adjustment a 3D scene was auto genereated from blender-osm in Blender, then saved as FBX and PNG files.
\\
This two files were imported inside Unreal Editor, applying auto detection of collision meshes. Simple collisions were used instead of complex even if they are known to be less optimized, they did not affect significantly the computation.
\\
At this point AirSim was ready to be used inside the georeferenced environment where new APIs calls were needed to extend UAV client with commands able to move the vehicle towards specified latitude and longitude coordinates. This was possible thanks to \textit{AirSimGeo} repository that extends AirSim’s “MultirotorClient” with new functions:

\begin{itemize}
	\item getGpsLocation: to retrieve gps position of UAV transforming from local coordinates to GPS georef ones
	\item moveToPositionAsyncGeo: to move asynchronously UAV towards a specified GPS position.
	\item moveOnPathAsyncGeo: to move asynchronously UAV towards a specified GPS waypoints.
\end{itemize}

Blender and Unreal Engine are both defined in a cartesian coordinates system (x,y,z) and therefore a projection must be made in order to translate passing from NED (North East Down) coordinates to them and finally to GPS coordinates. 

\chapter{Results} \label{cres}
This chapter is dedicated to the empirical results obtained experimenting some specific aspects of \acrshort{malrl} framework. Starting from \acrshort{gl}'s results related to Reinforcement Learning then the experiments focus on altitude assignment in \acrlong{s3dl}'s algorithm, exploring the existing trade-off between scalability, in terms of number of considered trajectories, and time efficiency.

\section{Results in \acrshort{gl}}
The notation used in the following chapter is described in \ref{pl1}.
\subsection{Evaluation Criteria}
Reinforcement Learning is evaluated comparing the results obtained when enforcing a random agent with the results obtained when using QLearning and K-steps QLearning algorithms. Plots show the reward function behaviour when increasing the number of episodes and tables compare the different total elapsed time for training.

\subsection{Experiments}
Three main experiments have been conducted, each one has been composed of learning and testing using in the order: a random policy maximizing the reward obtained, standard QLearning algorithm, and K-steps version of QLearning.\\
To compare the different learning algorithms each experiment has been conducted keeping unchanged both the sequence of initial positions $S_e=\{s_{0,0},s_{0,1},...,s_{0,4}\}$, where $s_{0,i}$ is the initial position of $i$-th trajectory in the experiment $e$ and the sequence of goals (desired final positions) $G_e=\{g0,g1,...,g4\}$. It should be highlighted again that for our experiments each agent should collect exactly one single goal to consider concluded the training episode, but a mode with multiple goals per episode is implemented although not tested. 
\\
\todo[inline]{Check numerical correctness wrt experimetns}
To acquire results covering different cases, each triplet of experiments have been repeated four times choosing each time a different couple of sequences <$S_e,G_e$> for every triplet.
\\

\todo[inline]{WHERE TO PUT THIS?}
Each experiment shared the following training parameters:
\begin{itemize}
    \item Seed: $111$
    \item Num. of episodes: $500$
    \item K value (K-steps $Q$-Learning only): $20$
    \item Max. num. of steps per episode: $10000$
    \item Decay factor (to decrease explore and learning rate over time): $\delta = 185$ 
    \item Min. Learning rate: $\alpha_m=  0.2$ 
    \item Learning rate: $\alpha = \max \{  \alpha_m, \min \{ 0.8, 1.0 - 
            \log_{10}[\frac{(episodeNum +1)}{\delta} ] \}\} $
    \item Min. explore rate: $\epsilon_m = 0.001$ 
    \item Explore rate : $\epsilon = \max \{  \epsilon_{m}, \min \{ 0.8, 1.0 - 
            \log_{10}[\frac{(episodeNum +1)}{\delta} ] \} \} $
\end{itemize}

Plotting the results obtained from a random agent will be useless considering that in all cases steps required to reach the goal were above the maximum allowed number of steps per episode.
The following are tables and plots obtained from training experiments with the above parameters.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/QFigure_1.png}
	\caption{	Reinforcement learning general scheme from Sutton, R.S. and Barto, A.G., 2011. Reinforcement learning: An introduction.\cite{suttonRL}	}
	\label{fig: rq1}
\end{figure}

\begin{tabular}{ |p{2cm}|p{4cm}||p{3cm}|  }
 \hline
 \multicolumn{3}{|c|}{\textbf{Time comparison}} \\
 \hline
 \textbf{<}$S_e,G_e$\textbf{>}     & \textbf{Algorithm} & \textbf{Time Required}\\
 \hline
  <$S_1,G_1$> & $Q$-Learning &  \\
 \hline
  <$S_1,G_1$> & K-steps $Q$-Learning &  \\
 \hline
 \hline
  <$S_2,G_2$> & $Q$-Learning &  \\
\hline
  <$S_2,G_2$> &  K-steps $Q$-Learning &  \\
     \hline
        \hline
  <$S_3,G_3$> & $Q$-Learning &  \\
\hline
  <$S_3,G_3$> &  K-steps $Q$-Learning &  \\
     \hline

 \hline
\end{tabular}


We start with the plot regarding the trend of the reward function, where same couple of sequences <$S_e,G_e$> have same colour.  


\section{Results in \acrlong{s3dl}}
For this section the focus was posed on the scalability and time efficiency of the algorithm under different conditions.

\subsection{Evaluation Criteria}
The results regarding the algorithm presented in 
\todo[inline]{REF TO ALGO}
are analyzed, comparing risk model results and changing training parameters such as considered radius for collision of points needed for moving a trajectory, along with the total number of considered trajectories.

\subsection{Experiments}


\section{Discussion}




\chapter{Conclusions and Future Work}

\chapter*{Appendix}


% bibliography
\cleardoublepage
\printglossaries

\phantomsection
\bibliographystyle{sapthesis} % BibTeX style
\bibliography{bibliography} % BibTeX database without .bib extension



\end{document}
