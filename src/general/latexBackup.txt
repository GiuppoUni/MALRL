% !TeX encoding = UTF-8
% !TeX program = pdflatex
% !TeX spellcheck = en_US

\documentclass[LaM,binding=0.6cm]{sapthesis}

\usepackage{microtype}

\usepackage{hyperref}
\hypersetup{pdftitle={Usage example of the Sapthesis class for a Laurea Magistrale thesis in English},pdfauthor={Francesco Biccari}}

% Remove in a normal thesis
\usepackage{lipsum}
\usepackage{curve2e}
\definecolor{gray}{gray}{0.4}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amssymb}
%\usepackage{algorithmicx}
\usepackage{color}

\usepackage{algorithm}
\usepackage{todonotes}
\usepackage{float}
\usepackage{subcaption}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{algcompatible}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\DeclareCaptionFormat{myformat}{#3}
\captionsetup[algorithm]{format=myformat}
\DeclareMathOperator*{\argmax}{arg\,max}
% \algblockdefx{MRepeat}{EndRepeat}{\textbf{repeat}}{}
% \algnotext{EndRepeat}

\usepackage[noend]{algpseudocode}

\usepackage[acronym]{glossaries}

\usepackage[utf8]{inputenc} % this is needed for umlauts
\usepackage[T1]{fontenc}    % this is needed for correct output of umlauts in pdf

% \renewcommand{\thealgorithm}{\arabic{chapter}.\arabic{algorithm}} 


\newcommand{\LI}[1]{\textcolor{purple}{LI: #1}}

\newcommand{\bs}{\textbackslash}
\newcommand{\R}{\mathbb{R}}

% Commands for the titlepage
\title{A Large-scale multi-UAVs Reinforcement Learning Framework with Multiple Abstraction Layers}
\author{Giuseppe Capaldi}
\IDnumber{1699498}
\course{Engineering in Computer Science}
\courseorganizer{Facolt\`{a} di Ingegneria dell'informazione, informatica e statistica}
\AcademicYear{2019/2020}
\copyyear{2021}
\advisor{Prof. Luca Iocchi}
\advisor{}
\coadvisor{Prof. Simone Scardapane}
\authoremail{giuppocapaldi@gmail.com}

\examdate{15 January 2021} 

\examiner{Prof. Marco Schaerf }
\examiner{Prof. Silvia Bonomi}
\examiner{Prof. Giuseppe Antonio Di Luna}
\examiner{Prof. Pierangelo Di Sanzo}
\examiner{Prof. Luca Iocchi}
\examiner{Prof. Christian Napoli}
\examiner{Prof. Roberto Navigli}
\examiner{Prof. Simone Scardapane}

\versiondate{\today}
%====================================%====================================
%								MY PACKAGES
%%====================================%====================================
\usepackage{amsmath}
\usepackage{verbatimbox}

\graphicspath{ {./figures/} }

\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\fname@algorithm~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
  \usepackage{subcaption}

%%====================================%====================================
%%   ACRONIMI
%%====================================%====================================


\makeglossaries

\newacronym{malrl}{MALRL}{Multiple Abstraction Layers Reinforcement Learning}
\newacronym{gl}{GL}{Grid Layer}
\newacronym{s3dl}{S3DL}{Simplified 3D Layer}
\newacronym{g3dl}{G3DL}{Georeferenced 3D Layer}
\newacronym{rwtl}{RWTL}{Real World Testing Layer}
\newacronym{cmvae}{CM-VAE}{Cross-Modal Variational Auto Encoder}
\newacronym{ATM}{ATM}{Air Traffic Management}
\newacronym{uav}{UAV}{Unmanned Aerial Vehicle}
\newacronym{uas}{UAS}{Unmanned Aircraft Systems}

\begin{document}

\raggedbottom

\frontmatter

\maketitle

\dedication{Dedicated to\\ my family}

\begin{abstract}

Unmanned aerial vehicles (UAVs) are rapidly spreading in many different fields, setting new challenges, risks, and opportunities in several real-life applications from civilian (surveillance, industrial monitoring, agricultural services, disaster relief, SAR) to military services (air exploration, battlefield surveillance, target localization, target tracking, target locking)\cite{Shakhatreh_2019}.\\
In many specific applications, for example in infrastructural inspection and damage assessment, using a UAV instead of traditional tools has been proven to be more efficient. Parallel to the spread of the use of single UAVs, many studies are focusing on the usage of groups of UAVs, cooperating or even competing with each other, to achieve higher efficiency, performance and resilience\cite{sysArch11}\cite{sysArch12}.\\
The usage of multiple UAVs imposes the need for coordination and formation control when cooperating for the success of a mission.
In the future multiple UAVs will coexist in the same large urban space, conducting different operations as a single vehicle or as fleets, probably without broadcast communication or cooperation among them, considering the presence of different actors and communication problems as delays or custom protocols to be interfaced.\\
In this context extending the view to a large-scale perspective, in terms of the number of vehicles, extensible to greater space dimensions could be beneficial especially to define and try to mitigate the risk of a mid-air collision. \\
Artificial intelligence (AI) and in particular Reinforcement Learning (RL) techniques could help in the choice of policies able to reduce the collision risk, defining trajectories computed to be sufficiently distant one from the other without changing starting or final positions.\\
%TODO check se c è sta parte
In collaboration with Universitat Politecnica de Valencia as well as other companies and agencies, a collision model was developed by researchers at Sapienza University of Rome coordinated by Prof. Luca Iocchi as part of BUBBLES project \cite{bubbles}. This model offers the opportunity to pass as input trajectories generated with the proposed framework and obtain as result risk indicators to be later compared for a risk reduction analysis.
\\
Virtual simulations can be used to find the policies that show the highest results, to be later tested and implemented in real life. 
Simulation is crucial for RL where several attempts (episodes) are required to train the agent, which would be too expensive to do with real agents, both in terms of time and economic costs.
\\
This master thesis will present methodologies, implementation, and evaluation details of a multi-layer framework called "\acrshort{malrl}" (\acrlong{malrl}) based on multiple abstractions levels, aiming to transpose and improve existing Reinforcement Learning implementations to a 3D Realistic simulated environment.\\
In this sense the framework and the algorithms presented in this thesis can be considered as a solid starting point towards further developments that could lead soon to the transposition of pure RL training, using abstracted environments, into real-world UAVs missions.
 
 %The use of UAV will grow even more in the future and our cities will be impacted by this considerable change. 
\end{abstract}

\begin{acknowledgments}
Ho deciso di scrivere i ringraziamenti in italiano
per dimostrare la mia gratitudine verso tutti coloro che mi hanno sostenuto in questo percorso accademico e di crescita personale.
Vorrei iniziare ringraziando il Prof. Luca Iocchi per avermi guidato con professionalit\'{a} e costanza, e per avermi affidato direttive chiare e utili allo sviluppo di questo e altri progetti. 
Sento di dover ringraziare anche i miei colleghi, in particolare Alessandro Trapasso, per il lavoro svolto assieme e Federico Fiorini, per le consulenze rigurdo agli UAS.
Un ringraziamento va anche al Dott. Damiano Brunori che ha messo a disposizione mia e del team di lavoro la sua esperienza e conoscenza in una costante collaborazione.
Infine un ringraziamento speciale, va alla mia famiglia che non ha mai smesso di sostenermi, perciò è a loro che è rivolta la mia dedica.

\end{acknowledgments}

\tableofcontents


\listoffigures
\listoftables
\mainmatter

\chapter{Introduction}

\section{The Demand for new Safe Multi-UAV Applications}

The use of multiple UAVs for visual monitoring can hold several practical applications, most of them are based on a representation of the object under investigation as imagery data. Cameras are very often used on UAVs, due to their low impact on the payload and possibilities associated with them, from the trivial pilot control using First Person View (FPV), with which the pilot can observe from the drone perspective in real-time, to monitoring, target detection, localization, classification, etc. \cite{mainSurvey}. 
%\LI{lasciare una riga vuota invece di \\ }

An imperative requirement for the integration of multi-UAV systems in all applications is the capability to ensure the safety of flights, within their mission space. To do so, conflict resolution and collision avoidance in UAV systems are considered an essential objective. The functionality of any air traffic management (ATM) system is to guarantee that collisions with other UAVs and/or targets can occur at a very low statistical probability, obtained as a risk measure \cite{pham}.

Environment sensing should be considered as a first design challenge to obtain essential data such as size, speed, geo-location of other UAVs, targets, obstacles. Secondly, decision-making techniques by ATM based on the collected data should be enforced in an efficient, time-responsive manner.

\section{Motivation}

Noticing an incremental growth of interest in Unmanned Aircraft Systems (UAS), being a technology of recent adoption, with many possibilities and risks for our future society that result in numerous engineering challenges, it naturally arises the question of what can Artificial Intelligence do in this direction. If indeed autonomous driving undeniably seems the near future for the car industry, it is also very likely that UAVs too will be involved in this challenge, which will require as the main priority, precise safety guarantees. They could be defined in many different ways but talking about collision risk minimization and potential conflict analysis, based on the position of the trajectories in the space seem to be the best one.

The personal interest in Reinforcement Learning algorithms and the discovery of \textit{AirSim} tool as an innovative tool, able to merge 3D editing with complex UAV simulations, were some of the elements that have been motivating this thesis.

Moreover the existence of BUBBLES project as a large multinational project, able to have a significant impact in the future of European aeronautics and to involve expertise from many different fields, including computer science and artificial intelligence, made a considerable contribution.
%Finally all the possible different characteristics of the situation such as type of UAV, model technical specification, power consumption, payloads, 2D or 3D trajectory paths, targets as moving or stationary objects should be taken into consideration.

\section{Proposed Solution}

%\LI{collision avoidance è solo l'ultimo stadio della risoluzione di conflitti, secondo me in tutta la tesi va generalizzato con "minimizing the collision risk"}

The need for minimizing the collision risk to enforce safe multi-UAVs mission executions in the same area of space is considered as an essential requirement of the project presented in this thesis. In this sense Reinforcement Learning techniques will make possible the generation of iterative 3D trajectories, representing different paths to reach the target. Techniques for minimizing the collision risk, in 2D, involve the use of a negative reward for the agent, when it moves over a previous trajectory (considered as the final path explored in the last episode of the previous iteration), and involve, in 3D, the use of an algorithm for vertical separation assignment. Vertical separation can be applied to trajectories generated using RL but also to any other set of trajectories inside the mission space. More specifically, we aim at collecting sets of different trajectories (simulating air traffic in the area) that are generated iteratively and considered as uniform distributions of points in the space, where the time component is ignored in the context of continuous or very frequent flights for every single route. Each generated trajectory will be affected by the agent's initial position, desired final position (coincident with the goal's position and unknown to the agent), trajectories generated by other agents until the current time instant, and obstacle positions.

These trajectories can then be used inside AirSim to guide realistic UAV actors that will follow them one by one, correcting the trajectories exploiting on top all the possibilities AirSim offers, from general Computer Vision using virtual cameras and sensors to Reinforcement Learning and DRL. 

\subsection{Scenario Characteristics}
As already mentioned,  UAVs may be involved inside a plethora of applications, leading to an equally large number of space scenarios that need to be restricted. For our purpose, a specific urban city context has been considered that is the portion of a city or industrial area presenting a grid plane for its streets, that needs to be patrolled and buildings. Many cities, especially modern metropolises like New York, present a grid plane structure. Later it will be clear the importance of this assumption for our scenario in terms of possible UAV trajectories.

The specific application can be imagined as the patrolling activity of streets in a city while searching for a target (specific vehicle or suspect) with an unknown position or in an industrial grid structure context to scan the area for the presence of faults.

\subsection{Abstraction Layers}
The complexity of a real-world scenario needs to be reduced by introducing an abstracted design of all the most significant elements involved: the environment with its obstacles and possible paths, targets, and agents. This is true especially for urban areas where the complexity in terms of obstacles in the flight of UAV agents is increased. 
Moving objects of any kind (especially at low altitudes) in areas populated by people, together with no-fly zones, restrictions, geo-fences makes urban spaces among the most challenging areas to let multiple UAVs flight. 

However, these are the areas where multi-UAV applications can become more useful, so this thesis aims to be the first step towards real-world modeling, abstracting some aspects that would not add information to the model or can extend it in the future, being implemented and customized inside a scalable framework for trajectories generation.  

\subsection{Iterative Generation of Trajectories}

    %\LI{spiegare che la produzione di traiettorie per il calcolo del collision risk giustifica la possibilità di generare queste traiettorie off-line e in maniera incrementale, poiché il risultato è comunque una stima probabilistica della frequenze di conflitti e di collisioni.}

In this thesis, a novel approach has been used for the generation of UAV trajectories where, instead of simultaneous parallel simulated flights of multiple UAVs, an offline iterative approach has been preferred. Here the generation of trajectories is incremental which means that in \acrshort{malrl} framework the flights are simulated one at a time and then sampled to produce fine-grained ordered collections of points, each one representing a different trajectory. Having a trajectory generator that is off-line and incremental is considered acceptable due to the objective this method will let achieve. Indeed trajectories will be eventually loaded and processed to obtain the corresponding collision risk measure. This value would be a probabilistic estimation of the collision risk based on the frequency of collisions and conflicts between the flights they represent.    

Trajectory generation is first applied in the two-dimensional space so that the trajectories are then passed to the algorithm responsible for the altitude assignment for each one, transposing the problem of minimizing the collision risk into the three-dimensional space.

Therefore the asynchronous offline nature of trajectory generation is conceptually detached from computational power limitations concerning online simultaneous generation, leading the generation to possible large-scale dimensions in terms of the number of trajectories. The experiments conducted for this purpose show a partial, though an essential, measure of \acrshort{malrl} scalability capacities leaving room for discussions on the argument and possibilities to extend its implementation, even into real-world contexts.


\section{Advantages of Multiple Abstraction Layers}

\acrshort{malrl} framework is composed of four abstraction layers. The first is an abstracted 2D grid plane representing the environment onto which reinforcement learning, in particular, $Q$-learning algorithm is applied and that lets generating an arbitrary number of trajectories in output. 

Moving from 2D space towards 3D space, it was designed an algorithm responsible for altitude assignment for each trajectory, keeping the focus on collision risk minimization.

The second layer is designed upon a 3D portion of space, built into Unreal Engine, delimited by fixed perimeter walls to restrict the area of simulation, it is a simplified replica of the city under study with blocks of the same size to simulate the presence of buildings. 

The third layer is conceptually an extension of the precedent, again, a 3D environment inside Unreal Engine, still delimited by perimeter walls but it has been built from scratch as a significantly larger and more realistic 1:1 replica of the city, geo-referenced using GPS coordinates.

The fourth and last layer involves a real-world deployment of the policies obtained and could be investigated in future works since the study of this thesis was mainly on the first three layers both for temporal and feasibility reasons.

\section{Objectives of Thesis}

%\LI{Metti prima un obiettivo più generale: generazione di traiettorie in modo scalabile rispetto a dimensione dell'ambiente, numero di UAV e complessità delle missioni.Traiettorie da usare per la valutazione del collision risk e per la decisione da parte dell'ATM delle strategie di separazione per una navigazione sicura.}
The main purpose of this project is to build a framework able to generate trajectories, in a scalable way, in terms of space dimensions, number of UAVs, and mission complexity. These trajectories will be useful for the evaluation of collision risk, giving more information and possibilities to decide on the UAV separation strategies, that make navigation safer, to the ATM authorities.     

The specific objectives characterizing this thesis are related to the design, development and analysis of such framework as a scalable and expandable platform, written in Python language, based mainly on Gym toolkit and AirSim simulator, to use reinforcement learning algorithms for real-world multi-UAV applications, avoiding collision risks.

The scalability of the framework should be in terms of the number of UAVs considered through the use of the already mentioned abstraction layers. Several factors involved in the altitude assignment algorithm influence this capability, from the radius to measure distance point-to-point to vertical separation slots dimension and even the tolerance value, defined later.

The extensibility of \acrshort{malrl} resides in the use of tools in common use such as OpenAI Gym and AirSim, both very present in numerous publications relative to reinforcement learning and UAVs autonomous flights. The modular design of \acrshort{malrl} as a framework leaves the possibility to extend individually each layer.

\section{Results of Thesis}

%\LI{Riassumere i risultati ottenuti: esempio: il framework sviluppato consente di generare many 3D trajctories in limited amount of time.....}

Several experiments have been conducted to analyze some aspects of \acrshort{malrl} framework, such as the quality of solutions obtained from $Q$-learning and K-steps $Q$-learning algorithms, as well as the scalability and computational time required in the altitude assignment phase for vertical separation.
What results will show is the ability of \acrshort{malrl} framework to generate many credible trajectories, depending on the 2D and 3D physical structure model of the area relative to the mission, in a limited amount of time. The choice of a proper RL algorithm, optimized for the defined scenario, will make possible to use RL as a tool to enforce horizontal separation that together with an algorithm designed to enforce vertical separation will lead to a set of trajectories characterised by a low collision risk measure.    
%to reduce computation time while altitude assigner showed to be considerably time efficient in our tests. 
% Di AirSim non parli?

\section{Organization of Thesis}

The rest of this thesis is organized as follows. \textbf{Chapter 2} reports related works, to give a theoretical perspective on prerequisites. \textbf{Chapter 3} provides the background in terms of software tools and implementation-oriented concepts. In \textbf{Chapter 4} the problems faced in each layer are formally defined. 
In \textbf{Chapter 5} architecture and design challenges related to the framework are discussed. \textbf{Chapter 6} contains details about implementation and development. In \textbf{Chapter 7} results from the training and conducted experiments are presented. \textbf{Chapter 8} concludes the thesis with potential ideas for future improvements.

%
%=====================================================================
%
\chapter{Related Work}
The majority of papers published on AirSim implement deep reinforcement learning algorithms, describe general characteristics of this recent platform, and compare it against other possible solutions. Studies on using AirSim to achieve collision risk minimization are easy to find but the majority focuses on dynamic collision avoidance, implemented in real-time through onboard computation by each drone.
Such methods can lead to a high level of safety but are also affected by some limitations, one among the others is battery consumption. Offline static collision risk minimization can be easier to implement and more efficient in contexts like the ones BUBBLES plans for the future, where UAV missions will be arranged with Aeronautical authorities and follow specific regulations.
 % particularly important challenge in the field, since considers static collision avoidance can be considered as already available in many commercial products.
%\todo[inline]{FONTI}
However, a systematic multi-layer approach exploiting specific geometric grid structure of cities to enforce 2D/3D trajectory generation was not found and seemed an interesting opportunity to explore the following themes.

\section{Reinforcement Learning}
Reinforcement Learning (Sutton et al., 1998) is an area of machine learning considered different both from \textit{supervised learning}, in which learning is obtained from a training set of labeled examples provided by a knowledgable external supervisor and both from \textit{unsupervised learning}, in which learning is obtained finding structures hidden in collections of unlabeled data. In this sense, RL shares with unsupervised learning the absence of correct examples to rely on. However, the goal of RL is to maximize a reward signal instead of trying to find hidden structures, something not directly addressed in unsupervised learning.

The basic idea of RL problems is capturing the most significant aspects of the real problem facing a learning agent interacting with its environment to achieve a goal. 
A learning agent will sense the state of its environment and take actions to affect it to achieve its defined goal(s). 

RL consists of three primary elements: (i) the agent (learning agent acting on the environment); (ii) the environment; and (iii) the actions. To formally describe the environment Markov Decision Processes (MDPs) are introduced, indeed the problem of reinforcement learning can be formalized as the optimal control of incompletely-known MDPs.

A deterministic MDP can be defined as: 
$$MDP=< \textbf{S},\textbf{A},\delta,r > $$
where  \textbf{S} is a finite set of states; \textbf{A} is a finite set of possible actions; $\delta$ is a transition function ($\delta: \textbf{S} \times \textbf{A} \xrightarrow{}\textbf{S} $); \textit{r} is a reward function ($r: \textbf{S}\times\textbf{A} \rightarrow{} \R $).

MDPs must follow the Markov property so: $x_{t+1} = \delta(x_t,a_t)$ and $r_t = (x_t,a_t)$. In some cases the reward function ca be also defined as: $r: \textbf{S} \rightarrow{} \R$.



\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{RL}
	\caption{	Reinforcement learning general scheme from Sutton, R.S. and Barto, A.G., 2011. Reinforcement learning: An introduction.\cite{suttonRL}	}
	\label{fig: rl}
\end{figure}


\subsection{$Q$-Learning}
$Q$-learning \cite{watkins1992q} is one of the most famous algorithms studied in Reinforcement Learning and one of the first breakthrough in this field.

When the agent ignores the environment, Temporal Difference methods can be used to solve the MDP problem. The environment is said to be unknown for the agent when the transition probabilities and the reward expectation are unknown.

$Q$-learning is an off-policy TD control algorithm that allows learning the \emph{quality}, or expected utility, of each state-action combination. That is, for each state, all the expected rewards obtained by taking each possible action at that particular state are estimated. This is represented as a state-action table called Q-table. A Q-table can be used to define a policy by always picking the action with the highest expected return.

More formally, $Q$ function can be estimated as: $Q: S \times A \to \mathbb{R}$. $Q$ can be modeled as a mapping table (initialized with some uniform values), whose value is updated at each time step of our simulations.

Here's how our Q-table is updated at each time step $t$:
\begin{equation} \label{eq:QUpdate}
\begin{aligned}
  Q(s_{t},a_{t}) ={} & \underbrace{Q(s_t,a_t)}_{\rm old~value} +
  \underbrace{\alpha}_{\rm learning~rate} \times \\
   &\times \left[
    \overbrace{\underbrace{r_{t+1}}_{\rm reward} + \underbrace{\gamma}_{\rm
        discount~factor} \underbrace{\max_{a}Q(s_{t+1}, a)}_{\rm
        estimate~of~optimal~future~value}}^{\rm learned~value} -
    \underbrace{Q(s_t,a_t)}_{\rm old~value} \right]
\end{aligned}
\end{equation}

where:
\begin{itemize}
	\item $\alpha\in[0,1]$ is the \emph{learning rate}, a coefficient that regulates how much the newly learned values will contribute in the update
	\item $\gamma\in[0,1]$ is the \emph{discount factor}, a coefficient that controls the weight of future rewards. Values closer to 0 will make our agent "short-sighted", considering only the immediate rewards.
\end{itemize}
Then the algorithm is the following:\\

\begin{algorithm}[H]
\caption{\textbf{$Q$-learning}}
\label{alg:$Q$-learning}
\begin{algorithmic}[1]
\Require
 	\State $S$ is a set of states
 	\State $A$ is a set of actions
 	\State $\gamma$ the discount reward factor
 	\State $\alpha$ is the learning rate
 	\State $n$ is number of episodes to run $Q$-learning
 	\State $\epsilon$, probability to take random action, rather than follow policy
\Procedure{$Q$-learning} {}
\\Initialize $Q(s,a)$ will all 0 utility values.
\For{each episode $e_i$ with $i=0...n$}
\\ \qquad Initialize $s$
\For{each step of episode}
\\ \qquad \qquad Choose $a_t$ from $s_t$ using policy derived from $Q$ with $\epsilon$-Greedy
\\ \qquad \qquad Take action $a_t$, observe reward $r$ and $s_{t+1}$
\\ \qquad \qquad Update Q-table using equation~\ref{eq:QUpdate}
\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
Each episode converges to a terminal state or is stopped after a fixed number of steps.\\
The action choice in Step 12 uses the exploration/exploitation policy (function)
$EEP(s_t,Q_t,S,A)$ that is defined as follows:
\[   
EEP(s_t,Q_t,S,A)=a_t= 
\begin{cases}
\underset{a\in A}{arg \max}\ Q_t(s_t,a_t) &\quad\text{\textit{exploitation} with probability}\  1-\epsilon \\
\ & \\
\underset{a\in A}{random(a)} &\quad\text{\textit{exploration} with probability}\  \epsilon
\end{cases}
\]
The above EEP function implements a policy denoted as the “greedy policy” where $\epsilon$ is often chosen as a small probability (i.e., 0.05).
 

\subsection{K-Steps $Q$-learning}
K-steps version of $Q$-learning is a modified version of standard $Q$-learning that updates Q-table with the aforementioned equation (Eq. \ref{eq:QUpdate}), not only for current state-action pair but also for the previous $k$ pairs before the terminal state. In other words the algorithm keeps memory of the last $k$ state-action pairs: $\{(s_1,a_1),...,(s_k,a_k)$\} s.t. $s_k$ is the second last state in the episode and the following state $s_{k+1}$ coincides with the terminal state.

So the new algorithm will result to be:

\begin{algorithm}[H]
\caption{\textbf{K-steps $Q$-learning}}
\label{alg:$K-steps Q$-learning}
\begin{algorithmic}[1]
\Require
 	\State $S$ is a set of states
 	\State $A$ is a set of actions
 	\State $\gamma$ the discount reward factor
 	\State $\alpha$ is the learning rate
 	\State $n$ is number of episodes to run $Q$-learning
 	\State $\epsilon$, probability to take random action, rather than follow policy
\Procedure{K-Steps $Q$-learning} {}
\\Initialize $Q(s,a)$ will all 0 utility values
\\Initialize $Queue$ as empty queue

\For{each episode $e_i$ with $i=0...n$}
\\ \qquad Initialize $s$
\For{each step of episode}
\\ \qquad \qquad Choose $a_t$ from $s_t$ using policy derived from $Q$ with $\epsilon$-Greedy 

\\ \qquad \qquad Take action $a_t$, observe reward $r$ and $s_{t+1}$
\\ \qquad \qquad Save $(s_{t+1},a_{t+1})$ in $Queue$ (\textbf{if} $\vert Queue \vert=k$, remove first element of $Queue$)
\EndFor
\For{ $(s_i,a_i),(s_{i+1},\cdot) \in Queue$ }  
\\
\qquad \qquad Update Q-table using equation~\ref{eq:QUpdate} where $s_t=s_i,a_t=a_i,s_{t+1}=s_{i+1}$
\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

The k updates of the Q-table could be very useful in our case to avoid the agent explore the already explored path. This algorithm will be compared to standard Q-learning in the results chapter. 

\subsection{Hierarchical Reinforcement Learning and Hierarchical Abstract Machines}
In Andrey Kurenkov's essay (\cite{kurenkov2018reinforcementflaw}) some limitations in the RL field, addressed as major limitations, are discussed. They include:
\begin{itemize}

\item Sample efficiency: data generation is often a bottleneck and current RL methods are often data inefficient;

\item Scaling up: the application of classic RL to the problems with large action and/or state space is infeasible (the curse of dimensionality);

\item Generalization: trained agents can solve complex tasks, but if we want them to transfer their experience to new (even similar) environments, most state of the art RL algorithms will fail (brittleness due to overspecialization);

\item Abstraction: state and temporal abstractions allow to simplify the problem since resulting sub-tasks can effectively be solved by RL approaches (better knowledge representation).

\end{itemize}
Major flaws in RL are relative to scaling issues, Hierarchical reinforcement learning (HRL) is a computational approach intended to address these issues by learning to operate on different levels of temporal abstraction.

HRL methods learn a policy made up of multiple layers, each of which is responsible for control at a different level of temporal abstraction. Indeed HRL extends the set of available actions so that the agent can now choose to perform not only elementary actions but also macro-actions, i.e. sequences of lower-level actions. Hence, with actions that are extended over time, we must take into account the time elapsed between decision-making moments. Luckily, MDP planning and learning algorithms can easily be extended to accommodate HRL.

The work on hierarchical learners dates back to at least 1952 when Ashby developed a gating mechanism to handle recurrent situations and repetitive environmental disturbances.
The early work in hierarchical planning aimed at building a gating mechanism that switches between known behaviors. A subsequent system proposed a feudal representation [67], in which a hierarchy of managers and submanagers controlled an agent.

A subsequent system proposed a so-called "feudal" (inspired by Medieval Europe's Feudal system) representation where two principles hold:
the first is information hiding, since the environment is observed at different resolutions, sub-managers only obey the sub-tasks they are set to without knowing whether it satisfies the higher-level managers’ goal;
the second is reward hiding since communication is made between managers and "workers" through goals (a reward is given for reaching them).
A noteworthy effect of information and reward hiding is that the managers only need to know the state of the system at the granularity of their own choices of tasks. They also don't know what choices their workers have made to satisfy their command, since it is not needed for the system set up to learn, managers only know the states at the granularity of their own choices of tasks, sub-managers don’t know the task their manager is set to and super-managers don’t know the choices their manager made.

A subsequent system proposed a feudal representation, in which a hierarchy of managers and submanagers controlled an agent. The aim of the feudal planner is not to pick a submanager, but instead to give an appropriate command to that submanager. Each level has exactly one manager. All the managers learn the values of possible commands they can give to their submanager, given the current state and the command given to them by their manager. At the lowest level, a command is just a primitive action.  

Unfortunately, the Feudal $Q$-learning algorithm is tailored to a specific kind of problem and does not converge to any well-defined optimal policy but it has paved the way for many other contributions.
% -------------------------- HAMS-----------------------------------


Another representation for specifying a hierarchical structure is the hierarchy of abstract machines (HAM).
\begin{definition}[Hierarchy of Abstract Machines]
A hierarchical abstract machine (HAM) is
defined as a finite-state automaton with each automaton node marked with one of the following:
\begin{itemize}
\item Action: Execute a primitive action in the environment.
\item Call: Call another abstract machine as a subroutine.
\item Choice: Non-deterministically select the next machine node.
\item Stop/Return: Halt the execution of the machine and return to the previous call node, if any.
\end{itemize}
\end{definition}
An abstract machine is essentially a specific representation of a partial policy, one in which either the action to be executed is known, or if in a choice node, the action must be decided among the possible ones. A hierarchical abstract machine has the added power to call other lower-level machines as subroutines — this leads to modular understanding and execution. 

Another way to understand a HAM is that it is a partial algorithm or program encoding our knowledge regarding various subproblems. For example, a simple way to traverse east could be to go east, until there is an obstacle. If there is an obstacle, back off and change direction, going north or south a few steps, and when the obstacle is cleared go back to going east.

This is easily encoded as a HAM, by having machines for going east, north, and south. The high-level machine will decide to go east, and when an obstacle is seen, a choice node, which selects between north and south machines, will come into play.

While options create more action choices by creating more complex actions, HAM too can constrain the set of allowed actions. Where task hierarchy can only specify a high-level goal (pseudo reward), HAM can also specify specific ground actions. Thus, HAM allows for a flexible way for the domain designer to specify domain-control knowledge. 

What is missing both in HAMs and HRL in general, is an explicit formulation of data computed in one layer and passed to the higher layer, something \acrshort{malrl} framework tries to enforce. \cite{BerliacHierachialRL2019,planMDP}



\section{Space-based Collision Avoidance Framework for Autonomous Vehicles}
In \cite{YU201837} a multi-agent collision risk minimization framework for autonomous cars is presented, with the focus on the design of a general high-level abstract architecture, less on the specific AirSim implementation.

In the paper the importance of collision avoidance in autonomous systems and in particular in autonomous car applications is addressed, mentioning well-known cases of autonomous car accidents. The context appears to be different from minimizing the collision risk for UAVs, in which we have more elements to consider, among the others, the third dimension (altitude), offering new possibilities for avoidance and also kinematics and collisions typologies, different from the proposed ones.

The paper focuses on properly capturing and accounting for the high variability of geometries,  shapes,  and sizes of the agents (e.g. 18 wheels truck vs. 4 doors sedan),  something critical in situations with a high risk of accident (e.g., intersection crossing). 

Spatial-temporal collision avoidance algorithms are defined, their goal is to ensure an accurate prediction of the collision and correct decisions on the appropriate steps to avoid its occurrence.  
Since the implementation and simulation in AirSim of the proposed solution were reported to be under development relative to the study case of cars approaching an intersection in low light conditions, not much information was found about the argument.  

This paper shares with the current thesis the accounting for the importance of collision avoidance in a near future of autonomous vehicles expansion and for the idea of designing and developing a complex framework able to exploit the dynamic models, collision detection capabilities, 3D realistic rendering, and ease in virtual 3D world construction of AirSim.



\section{Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal Representations}
In \cite{vaeRL} it is proposed a method to train UAV agents to enforce effective first-person view (FPV) drone navigation in the real-world using simulation-only data from AirSim. Navigation is put in the contest of autonomous drone racing competitions where a flight is intended as navigation through gates in a track.

Researchers built a cross-modal Variational Auto-Encoder (CM-VAE) framework in charge of encoding raw sensor data and labeled task-specific state information into a latent representation.
This representation is then leveraged by a control policy, trained using imitation learning to map latent variables into velocity commands for the UAVs. 

The robust visuomotor policies learned using simulated data are then demonstrated to be applicable in real-world scenarios. To do so, sim-to-real transfer learning methods were applied, managing to escape from difficulties of collecting labeled real-world data which requires specialized equipment and from poor sample efficiency of end-to-end methods. During deployment, the representation uses only raw images taken from a USB camera, without access to the ground-truth gate poses.

The experiments conducted and presented in this paper (available also on \url{https://youtu.be/VKc3A5HlUU8}) showed interesting results considering the divergence of some real-world scenarios from the simulated ones.
Moreover, the introduction of challenging elements like snowy condition, different background colors, visual distractions (red stripes on the ground), to make image recognition harder or complex track structures, strong wind gusts (up to 20 km/h), to make navigation harder, showed the incredible capability of avoiding overfitting to the specific characteristics of incoming data.

This approach shares with \acrshort{malrl} framework the final objective of the deployment in the real world of simulated and virtually trained data, demonstrating its feasibility in very different real-world locations and conditions.   


\section{BUBBLES Project}

Some motivations for this thesis came from the European project BUBBLES \cite{bubbles}, 
coordinated by Universitat Politècnica de València, with the participation of  DIAG, Sapienza University of Rome, and other European universities and companies.

The BUBBLES project aims at defining separation minima and methods to UAS flying in the VLL (Very Low Level) SES (below 150 m) to improve the overall performance and safety guarantees. It will develop algorithms to compute the collision probability between Unmanned Aircraft Systems (UAS) using separation minima to keep it under acceptable levels. Moreover, the project itself will investigate how AI can contribute to dynamically managing these minima using different separation methods and agents (from centralized strategic deconfliction to distributed self-separation). BUBBLES will follow an operation centric, risk-based approach, assigning separation minima and methods depending on the UAS ConOps and the available U-Space services. BUBBLES will also extend the concept of Performance-Based CNS to the UAS operations to draft safety and performance requirements.

BUBBLES will develop Artificial Intelligence (AI) based algorithms to compute the collision risk of UAS leading to separation minima and methods so that a Target Level of Safety (TLS) stated in terms of the overall probability of collision can be defined and maintained.

These algorithms will be applied to a set of generic ConOps for UAS operations defined by BUBBLES, detailed enough to cover most of the envisaged applications, but generic enough not to be linked to any particular one. They will be classified in terms of risk using the SORA (Specific Operations Risk Assessment) methodology. SORA is a multi-stage process of risk assessment aiming at risk analysis of certain unmanned aircraft operations, as well as defining necessary mitigations and robustness levels.

These separation minima and methods will be assigned to the ConOps using AI techniques, leading to the definition of a set of generic OSEDs (Operational Service \& Environment Description) from which safety and performance requirements for the CNS systems will be derived applying a performance-based approach.

Bubbles is a large project involving many concepts related to UAS and the aeronautic field in general. As an important contribution to our thesis, it helps us define some concepts we will use throughout the entire dissertation. Will be used the term "collision" to indicate the physical impact of a drone against some physical static structure or moving object, while "conflict" will indicate the problematic distance between two or more UAVs (represented by geometric points), considered as a "potential collision" in this sense.

\section{Vertical Separation} \label{verSep}
Vertical separation \cite{verSep}, as the name suggests, is the concept used in air traffic control to define the minimum distance between two aircraft to reduce the risk of collision, as well as prevent accidents due to secondary factors, such as wake turbulence.  Air traffic controllers apply rules, known as separation minima to reduce this risk. Two aircraft following separation minima are said to be "separated" whereas if separation is lost they are said to conflict.
The minima vary depending on the relative size of the two aircraft and the block of airspace considered. 

ICAO specifies minimum vertical separation for IFR flight as 1000 ft (300 m) below FL290 and 2000 ft (600 m) above FL290, except where Reduced Vertical Separation Minima (RVSM) apply. Most national authorities follow a similar rule but may specify a different level at which the rule changes.

If during an emergency, it is not possible to ensure that the applicable horizontal separation can be maintained, emergency separation of half the applicable vertical separation minimum may be used. This means that a 1000 ft vertical separation minimum may be reduced to 500 ft and a 2000 ft vertical separation minimum may be reduced to 1000 ft.
The use of emergency separation is described in ICAO Doc 4444, 15.7.1.

These concepts are common to what BUBBLES aims at, developing a safe UAV traffic management system. The techniques developed in this thesis will thus partially contribute to BUBBLES objectives. 






\chapter{Background}
    In this chapter, it will be explained the set of software tools and implementation concepts involved in the process of building \acrshort{malrl} framework.  
\section{OpenAI Gym}
Gym is an open-source toolkit for developing reinforcement learning algorithms in Python. It makes no assumptions about the structure of the agent and is compatible with any numerical computation library such as TensorFlow or Theano.

The gym library is a collection of test problems (environments) that can be used to train and test any reinforcement learning algorithm. These environments have a shared interface, allowing to write general algorithms and to choose if using predefined environments, for well-known problems as "CartPole" or build a new environment from scratch, as it has been done for \acrshort{gl} of \acrshort{malrl} framework.

Gym usage as a tool is growing rapidly in the RL research field due to the possibilities it offers to mitigate two significant limitation for RL:
the need for better benchmarks and the Lack of standardization of environments used in publications.

In supervised learning, progress has been driven by large labeled datasets of common use like ImageNet, CIFAR, and others. In RL, the closest equivalent would be a large and diverse collection of environments. However, the existing open-source collections of RL environments don’t have enough variety and they are often difficult to even set up and use.

Talking about different implementations of very similar environments, it is easy to notice subtle differences in the problem definition, such as the reward function or the set of actions, which can drastically alter a task’s difficulty. This issue makes it difficult to reproduce published research and compare results from different papers, and it's something OpenAi Gym tries to mitigate.

\section{AirSim}
AirSim is a 3D simulator for drones, cars, and more, built for Unreal Engine software (enabling also Unity editor but only in experimental releases). AirSim is open-source, cross-platform, and supports \textit{software-in-the-loop} simulation with popular flight controllers such as PX4 and ArduPilot and \textit{hardware-in-loop} with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin and can be easily integrated into any Unreal environment (experimental support is present also for Unity, even if not tested in this thesis). 

AirSim aims to be a platform for AI research to experiment with deep learning, computer vision, and reinforcement learning algorithms for autonomous vehicles. For this purpose, AirSim exposes APIs to retrieve data and control vehicles in a platform-independent way.

AirSim is in continuous development and recently has added multi-vehicle simulation, opening new scenarios in the field of Multi-Agent Reinforcement Learning. 

\section{Blender}
Blender is a free and open-source 3D creation suite. It supports the entirety of the 3D pipeline; modeling, rigging, animation, simulation, rendering, compositing and motion tracking, video editing, and 2D animation pipeline.

For this thesis, due to the priority given to other aspects, a very limited part of this powerful software was used, when exploiting the presence of a free plugin enabling 3D OSM data rendering. 

\section{Georeferencing}
Georeferencing is usually defined as the process of transforming the internal coordinate system of a dataset into geographic coordinates as latitude, longitude, and altitude (GPS). In the case of a picture (2D space), georeferencing can be described as the set of transformations needed to assign a tuple of latitude, longitude, and optionally elevation information to a pixel or a group of pixels. Similarly, georeferencing could be applied to a 3D space model transforming its internal coordinate systems into GPS coordinates.

It it common knowledge that the earth is an irregularly shaped ellipsoid and in cartography, an ellipsoid coordinate system (lon, lat) is used to define positions, but very often a 2D visualization is desired where the Cartesian coordinate system holds. To do so a projection coordinate system is used to project the ellipsoidal shape of earth onto a flat two-dimensional Cartesian coordinate plane. However, every projection has intrinsic distortion that may affect the area and the shape of real-world objects it aims to represent. 

To georeference, a virtual 3D space GPS accurate geographic data sources are needed to be later transformed (even scaled) to local coordinates in 3D space.
For this purpose, it was used OpenStreetMap. 


\section{OpenStreetMap}
OpenStreetMap (OSM) is a free, editable map of the whole world that is being built by volunteers largely from scratch and released with an open-content license. The OpenStreetMap License allows free (or partially free) access to its map images but more importantly, it allows access to all of its underlying map data. The project aims to promote new and interesting uses of this data. It is useless to explain the details involved in the use of open-source geographic data against proprietary, copyrighted software such as Google Maps, but it should be noted that OSM is not subject to any licensing fees or contractual restrictions.

OSM map is accessible to everyone and in many countries, OpenStreetMap is used as an everyday viable alternative to other map providers. The main disadvantage of OSM is its not finished state, however, the absence of data is limited to remote regions and the growing community is very active. \cite{geoRef}\cite{OSM}

\section{K-Dimensional Trees}
Finding all neighbors of a point inside a given sphere of the fixed radius in 2D or 3D space is an integral part of many approaches using three-dimensional data. Choosing the most efficient data structure and algorithm to range query these points becomes crucial. Among the many possibilities, range trees and KD-trees constitute both valid data structures. It will be discussed the latter since the choice fell on it and its advantages over the former.\\
K-D trees (also called as K-Dimensional trees) were invented in the 1970s by Jon Bentley, they are binary search trees where data in each node is a K-Dimensional point in space. 
Since they are a space partitioning data structure for organizing points in a K-Dimensional space, they are often extended and implemented into 2D or 3D space simulations.

Consider the following problem in 2D space (but it applies also for higher dimensions): given a large set of points \textit{S} compute the subset of \textit{S} of points within a given range distance from a given point \textit{p}. 

K-D trees allow avoiding a trivial "brute-force" approach, calculating the distance of \textit{p} from every point in \textit{S} through splitting the space into subspaces. Every non-leaf node in the K-D tree divides the space into two parts, called "half-spaces".

Points to the left of this space are represented by the left subtree of that node and points to the right of the space are represented by the right subtree. 

%Each level has a “cutting dimension” • Cycle through the dimensions as you walk down the tree. • Each node contains a point P = (x,y) • To find (x’,y’) you only compare coordinate from the cutting dimension is x’ < x?


\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{kdtree}
	\caption{
		A 3-dimensional k-d tree. The first split (red) cuts the root cell (white) into two subcells, each of which is then split (green) into two subcells. Finally, each of those four is split (blue) into two subcells. Since there is no more splitting, the final eight are called leaf cells.
	}
	\label{fig: kdtree}
\end{figure}

%
% PROBLEM
%
\chapter{Problems Definition} 
\label{chap:pdc}
 This chapter explores in detail the main elements composing \acrshort{malrl} framework, called "abstraction layers" or simply "layers", from a theoretical and empirical problem definition perspective.
 
 For an accurate problem definition, it is important to describe the characteristic structure, objectives, and issues that distinguish each layer. As a convention, a different name for each layer is used: the first layer, which is the first one modeling and processing the input data, is called "\acrlong{gl}" (\acrshort{gl}) due to the grid plane structure of the environment; the second layer is called "\acrlong{s3dl}" (\acrshort{s3dl}) cause data obtained from the previous layer are processed and included in this layer that makes use of a 3D environment (in Unreal Engine) that is a simplified reconstruction of the world in analysis; the third layer is called "\acrlong{g3dl}" (\acrshort{g3dl}) since the use of OpenStreetMap data to obtain a GPS georeferenced world (in Unreal Engine); the last layer is the "\acrlong{rwtl}" (\acrshort{rwtl}), that as the name suggests is relative to a (future) implementation, which provides the deployment of results obtained in the other layers in a real-world contest.
 
 The order in which abstraction layers are designed and presented (first layer to fourth) follows a top-down approach, i.e. from the general modeling of the environment with a top view on it, considering only two dimensions, up to a specific 3D realistic simulation and even the real world itself. Hence the concept of "abstraction", since each part of \acrshort{malrl} involves methods and methodology related to a different level of abstractions of the real world, while the term "layers" highlight the systematic structure of \acrshort{malrl} as a homogeneous architecture, composed of interconnected parts. 
 
\section{Grid Layer} \label{glpl}
The \textbf{\acrlong{gl}} is the most abstract layer and involves a 2D approximation of the city of choice. To conduct experiments using MALRL the chosen location to be modeled was a square-shaped portion of Barcelona's city map (Fig. \ref{fig: topMaps}). To model the 2D Environment as accurately as possible some measurements were made. The chosen district portion, situated in the historical center of Barcelona, presents a grid plane urban structure particularly symmetrical and this eased the general design of 2D and 3D maps. It should be noted that many cities in the world, or at least many districts, share a similar grid plane structure for which MALRL could be useful.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{topMaps.png}
	\caption{
		Top view from Google Earth of the square portion of Barcelona used in the experiments.
	}
	\label{fig: topMaps}
\end{figure}

In the area delimited by chosen latitude and longitude bounds, streets and buildings were measured through satellite images from a top view perspective. Streets appeared to have a width of approximately 20 meters whereas almost all buildings in the chosen portion occupy a surface equal to 120 square meters (Fig. \ref{fig: topMapsDet}).

\acrshort{gl} uses a 2D grid space composed of square blocks where each square has a side representing 20 meters in the real world, so a single row or column of blocks is enough to model a street whereas buildings are represented as $6 \times 6$ blocks. Buildings and streets are the only map elements represented in these abstraction layers but other elements (e.g dead-ends, no-fly zones,...)  are easily implementable. The use of precise measures for the 2D model is out of the purpose of this thesis, also considering the possibility to reduce the block scale to represent more precisely the real world. However in that case a trade-off between 2D space scale granularity and computational complexity needs to be taken into account. 


\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{topBarcCommented.png}
	\caption{
		Top view from Google Earth of the square portion of Barcelona used in the experiments, showing measurements.
	}
	\label{fig: topMapsDet}
\end{subfigure}%
\hspace{5mm}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{figures/measures.jpg}
  \caption{Detail of a building in GL, showing scaled measures of blocks from map in Grid Layer.}
  \label{fig:measures}
\end{subfigure}
\caption{Top view from real-world Barcelona district (a) and from layer one grid environment (b). }
\label{fig:layerone}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/mazeCommented (3).jpg}
	\caption{
Screen of \acrshort{gl} rendering the environments with all its elements, the starting point as a blue cell (A), the goal agent needs to collect (B), the agent moving cell-to-cell (C), the group of blocks representing buildings (D).
	}
	\label{fig: mazeRl}
\end{figure}
%TODO calcs of this complexity increase

\subsection{Goal Collection in Maze} \label{pl1}
Starting from the first layer, what is required as input to this layer is the number of trajectories we want to generate, the width ($w_{space}$) and the height ($h_{space}$) in meters of the rectangle chosen as the portion of the city to be represented, $w_{cell}$ and $h_{cell}$ as dimensions of a single board block (or cell), height ("height", "z" and "altitude" terms are used with the same meaning for this thesis) will be considered in the next layers since we are now in a bi-dimensional space.

To ease the experiments it was chosen a value of twenty meters both for $h_{cell}$ and $w_{cell}$.

Maze problems are well known and often used test-bed to employ reinforcement learning algorithms. They are derived from grid-world problems, one of the first problems to explore while studying Reinforcement Learning.
%The presented framework allows customizing maze dimensions and composing elements to let the future user analyze different conditions relative to the mission of UAVs. \\
The problem to be defined was considered to be well suited for a grid world environment: a fixed number $\lceil \frac{w_{space}}{w_{cell}} \rceil \times 
\lceil \frac{h_{space}}{h_{cell}}\rceil $ of cells compose the board, each element (agent, obstacle portion or goal) occupy exactly one cell. As already specified to obtain greater precision while modeling the environment, the map scale could be decreased, in this case, a single agent could require more than one block. In our experiments scale is fixed to be as one cell per element.

The state of the agent at time-step $t$ is composed of row and col numbers of the cell it is occupying:
$$s_t=(r,c),$$
where $r \in [0,width-1]$, $width = \lceil \frac{w_{space}}{w_{cell}} \rceil$, $c \in [0,height-1]$,$height = \lceil \frac{h_{space}}{h_{cell}}\rceil$  and $t \in [0,numSteps]$.      

The agent starts from a fixed state $s_0$ and can move inside the board to a new state in the following directions: up, down, left, and right. Every trajectory to be generated has its specific $s_0$, but they all share the same goal positions. 
At each time step $t$ the agent can remain at its state (cell) or move to a new one doing an action:
$$ a_t \in [0,3],$$
where each different value of $a_t$ is equivalent to a direction. 

The goal is defined as $g \in G$, $G$ is the set of goals to be collected to consider an episode concluded, i.e. when $\exists \ t\  s.t.\  s_t=g$. G can have unitary cardinality or greater than one, in that case, an episode is concluded when all goals are collected i.e. when $\exists\  t\ s.t.\  s_t=g\  \forall g \in G$. From now on it will be considered the case when G has unitary cardinality, i.e. $\exists\ !\ g$ being a single static goal, per each episode.

The problem of avoiding previously generated paths is addressed through the memorization of previously generated trajectories, keeping in memory a data structure for cells already visited, i.e. $visitedCells$. Then the reward function will discourage policies in which the agent moves over an already visited cell.   
%The cells at $g$ position are cells in which UAV is supposed to consider completed its mission so eventually the agent will position itself over this cell when this happens the episode is considered completed.
The reward function computes a reward for each step $t$ depending on the action done and the state in which the agent is:
\[   
r_t(s_t,a_t) = 
\begin{cases}
1000 &\quad\text{if }  s_t = g \quad\text{(agent reached the goal)}\\
-0.1 &\quad\text{if }  s_t = s_{t-1} \quad\text{(agent did not move)}\\
-5 &\quad\text{if }  s_t \in visitedCells \quad\text{(agent moved over previous paths)}\\
-0.1/(width\cdot {height}) &\quad\text{otherwise} \quad\text{(agent moved)}
\end{cases}
\]
\\
What is obtained when reinforcement learning is terminated is a set of trajectories: $$T^{l1} = \{T_{0,n_0}^{l1},...,T_{m,n_m}^{l1}\}$$ where $T_{i,n_i}^{l1}= \{s_{0},s_{1},...,s_{n_i}|s_{t} <_t s_{t+1} \}$ and $s_t$ has been already defined. It should be highlighted that a trajectory is considered as an ordered set of points, sampled in the space at a constant space frequency, in our case of value $h_{cell}=w_{cell}=20$ meters. We will omit details in this sense but should be sufficient to denote the presence of a simple interpolation procedure, to increase the number of points on the trajectory, as well as simple translation procedure due to the agent considered to be in the center of a cell. This procedures are enforced when moving from discrete to continuous space for the points, which is when passing from \acrshort{gl} to \acrshort{s3dl}. 

\section{Simplified 3D Layer}
The \textbf{\acrlong{s3dl}} sees the introduction of a new dimension since the altitude is considered from now on in every other layer. The other significant change is in the space to which points composing a trajectory belong. Indeed in \acrshort{s3dl} points belong to continuous space whereas in \acrshort{gl} we had a discrete space, related to the use of a grid plane. Between every couple of points $s,s' \in T_i^{l1}$ a basic interpolation was made, sampling at a fixed distance rate $ds$ the segment going from $s$ to $s'$ obtaining a number of new points $np$ equal to:
$$np=\frac{d(s,s')}{ds}-2$$ this will help in altitude assignment phase. 

In \acrshort{s3dl} the use of AirSim tool enables the possibility to create a 3D environment built to emulate again the same portion of the city under consideration but enabling static collision risk minimization based on vertical separation.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/3dPlot.png}
	\caption{
		Example of trajectories after altitude assignment.
	}
	\label{fig: 3dPlotEx}
\end{figure}

Real buildings now are no more represented by a square-shaped group of cells but instead by solid meshes of the same height positioned in the 3D space following the position of the previous layer but in scale 1:1 so now a simple conversion from cells to x,y continuous coordinates is implemented.

%\todo[inline]{how to express time restriction}
To take advantage of the AirSim tool an approach based upon \cite{vaeRL} has been conceptually designed and partially implemented but not tested due to time restrictions. So in addition to blocked-shape buildings, \acrshort{s3dl} tries to solve persistent collisions between UAVs and obstacles introducing red-colored highly visible gates, similar to the ones present in the aforementioned paper and showed in fig \ref{s3dlFig}. They can be used to train the agent to correct the trajectory only for the parts that seem to be problematic.


\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/blockSim.png}
	\caption{
		Screen from simplified 3D environment in Unreal, with corrective gates example (red squares) and invisible walls going from pillar to pillar. 
	}
	\label{fig: s3dlFig}
\end{figure}


\subsection{Altitude Assignment}
In the previous layer, every transaction was generated penalizing eventual overlapping trajectories in the XY plane, obtaining $T^{l1}$. However, this is not sufficient to guarantee complete conflict-free trajectories even if increasing \textit{width} and \textit{height}, increasing the number of cells for the width of the street, distancing the initial position of agents are all factors that can help in this sense. This is true especially for environments in which streets can hold a few numbers of paths or even just one path per street (like in our experiments).
Since $\forall \ T_i \in T^{l1}$, $T_i$ is missing the coordinate for the Z-axis, not only to decrease the risk of collision but also for the need of displaying the trajectory and let the agents follow it (this is done in \acrlong{s3dl} and \acrlong{g3dl}).

In this layer the third dimension was exploited for enabling the assignment of an altitude value to each trajectory, depending on possible collisions. Now collision detection is no more based on checking if two trajectories (set of cells) overlap but on checking instead if the two trajectories (now points in the Cartesian 2d space) have a Euclidean distance below a certain threshold.

The algorithm provides parameters to set: the \textit{radius}, equals to the threshold under which the euclidean distance $d(s_t,s_t')$, $\forall\ s_t \in T_1, s_t' \in T_2$, where $T_1,T_2 \in T^{l1}$, is considered as a (potential) collision between $T_1^{l1}$ and $T_2^{l1}$; the \textit{minimum} and \textit{maximum altitude} values defined as boundaries out of which flights are not permitted; the \textit{separation} value meaning the constant distance along Z-axis used to separate any two colliding trajectories.

The following will be a formal description of the problem: we have a box B with dimensions $w_{s} \times h_{s} \times (M_{a}-m_{a})$,
where $w_{s}\equiv w_{space}$ was already defined as well as $h_s \equiv h_{space}$, while $m_{a}$ and $M_{a}$ are respectively minimum and maximum altitude (z coordinate value in AirSim in meters).

No trajectory is allowed to be generated out of block B space. The origin of the axis is considered to be located in one of the vertices at the base of the block. In this layer the \textit{i-th} flight trajectory composed of n points (2D cells coordinates) in input is: 
$$T_{i,n}^{l1}=\{s_{0},s_1,...,s_{n}|s_{t}<_t s_{t+1} \}$$
where $a<_t b$ means $a$ was generated at timestep $t_a<t_b$, \\$s \in \R^2$, $ i = 0,1,...,\vert T^{l1}\vert $, $T^{l1}=\{ T_{0,n_0}^{l1},...,T_{m,n_m}^{l1} \}$.

Given a value $r \in \R$ (\textit{radius}), the algorithm should transform each $T_{i}^{l1} \in \R^2$ into $T_{i}^{l2} \in \R^3 $, assigning a different altitude value $z \in [m_a,M_a]$ to each single point $s_t \in T_i^{l1}$, obtaining:
$$ T_{i,n}^{l2}=\{(s_{0,i},z_{0,i}),...,(s_{n,i},z_{n,i}) | (s_{t,i},z_{t,i})<_t (s_{t+1,i},z_{t+1,i})\}$$ 
and 
$$ T^{l2}=\{ T_{0,n_0}^{l2},...,T_{m,n_m}^{l2}  \}$$

%Already said
%The $r$ value (\textit{radius}) introduced before is used by the algorithm since a potential collision between two flights e.g. flight $f1$ and flight $f2$ is considered to happen in case we have at least two points $s_{f1} = (x_{f1},y_{f1})$ and $s_{f2}=(x_{f2},y_{f2})$ in the same cartesian xy plan, so at same height where $d(x_{f1},y_{f1},x_{f2},y_{f2}) < r$, with $d(x_{f1},y_{f1},x_{f2},y_{f2}) = \sqrt {\left( {x_{f1} - x_{f2} } \right)^2 + \left( {y_{f1} - y_{f2} } \right)^2 }$

Now the trajectories generated using the algorithm ($T^{l2}$) can be displayed in the environment and followed one by one (or simultaneously if the computational power is sufficient) by UAV actors in AirSim. If a collision with a building or obstacles of any kind is encountered the flight is stopped. 

Here is possible to have a notion of the position in which the collision happened, aiming to let the agent auto-corrects the trajectory. This can be done through the use of Deep Reinforcement Learning or other learning approaches that exploit input image data from the on-board camera images to avoid obstacles while following a given trajectory. In particular, referring to \cite{vaeRL}, positioning gates (as the one in Fig. \ref{fig: s3dlFig}) could let the agent move away from the obstacles giving priority to the velocities encoded by the control policy using the available pre-trained (or custom trained) weights for the feature encoder. 

This experimental method will be addressed as a "patch" correction from now on.
\section{Georeferenced 3D Layer}

The \textbf{\acrlong{g3dl}} makes use of 3D trajectories in input from the above layer (\acrlong{s3dl}) with a similar approach to correct trajectories but is based on a new environment, GPS accurate and realism oriented, while the previous environment is simplified for the sake of data acquisition and efficiency in general. 

Thanks to the availability of OpenStreetMap data it was possible to download accurate GPS .osm files to georeference the local coordinates of Unreal Engine, so in this layer, it is possible to use latitude, longitude, and altitude values to identify points in the environment.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/georef.png}
	\caption{
		Screen from environment georeferenced to Barcelona, with corrective gates example.
	}
	\label{fig: georef}
\end{figure}

The possibility to introduce buildings with complex or irregular shapes (e.g. the Sagrada Familia basilica) and with a deviation of measures due to georeferencing w.r.t. approximated measures of the previous layer are both elements to take into account. This allows using again RL techniques but as localized "patches", correcting only the parts of the trajectories that are problematic. Similar to what has been defined in the previous layer about trajectory correction, the introduction of gates, Unreal actors strategically posed in the environment, could help to solve the problem.  

The increased realism of this new environment grants the possibility to provide significantly more realistic images to the algorithm in charge to analyze them and encode them into velocities to deviate from the problematic trajectory. 

\subsection{Final Trajectory Correction}
This layer enables the possibility to correct trajectories that are affected by the presence of new elements or by the absence of past elements in the environment concerning the previous abstraction layer. The presence of static obstacles not represented in \acrlong{s3dl} but present in \acrlong{g3dl} can determine an invalid trajectory resulting in a collision in AirSim. On the contrary, the absence of obstacles considered in \acrlong{s3dl} can determine very inefficient trajectories.

These limitations can be formulated into the following problem: the UAV agent is following a given trajectory in 3D given by the previous layer in AirSim. When a collision with any obstacle happens, the trajectory needs to be corrected to avoid the obstacle in future runs. So as soon as the collision event is triggered, the position of UAV in space $s_t = (lat,lon,z)$ is saved in memory as a tuple composed of latitude, longitude, and altitude (considered as absolute altitude, where the ground level is Z of zero value) coordinates. Then two constants are chosen: $N$,$M \in \mathbb{N}$ representing distances as number of time steps before and after current $t$ s.t. $\exists$ $s_{t-N}$,$s_{t+M} \in T_i^{l2}\in T^{l2}$ waypoints, otherwise $s_0$ and/or $s_{n_i}$ are taken instead. Once identified $s_{t-N}$,$s_{t+M}$, or their substitutes, they are saved and used respectively as initial and final positions for a new RL algorithm considered as a "local" problem solver, from now on generically called "patch system".

Indeed the problem of trajectory correction is localized to the problematic part of trajectories, replacing waypoints that resulted to be problematic (conflict), potentially causing a collision, with new waypoints generated by testing the policy obtained through local reinforcement learning.

The values of $N$ and $M$ variables are increased in case of persistent collision after the learning phase, to let the agent explore from initial positions or final positions more distant in time. In this layer, the set of previously generated trajectories in 3D $T^{l2}$ is modified into a new set of corrected trajectories $T^{l3}$ ready to be passed to the next layer. 

% this goes in solution...
%To solve these issues a "patch system" based on RL has been designed. When a collision occurs, the framework stops the simulation and save the position.

%\section{Problem in \acrlong{s3dl} and 3 }
%TODO remove this part
%Both in layers 2 and 3 we can define the problem to be solved as the following: in a 3D continuous space the agent is a multirotor vehicle, the dynamic of which is simulated using AirSim. When in "\textit{crab-mode}", the mode in which the experiments were conducted, the agent can move at a fixed height along a single axis among x and y, keeping the vehicle front always pointing in the direction of travel. In this mode UAV can reproduce trajectories obtained in \textit{\acrshort{gl}}, keeping a consistent mode that limits the capability of movement of a UAV but can be considered useful when UAVs paths are constrained by high buildings or the mission requires paths following streets network.\\ In both layers previous trajectories are displayed as segments in 3D space connecting the points extracted from the previous phase of 2D reinforcement learning. After displaying these trajectories, points are stored as K-D trees to ease the computation when analyzing possible collision with the agent.
%TODO parla dell'interpolazione

%So given a set of 2D trajectories (ordered set of vectors of dimension 2), a safety distance measure (in meters), a minimum and maximum accepted height, the goal is to generate a new set of 3D trajectories which keeps unchanged x,y coordinates for each point but have z coordinate computed to be the minimum distance equal to the safety measure in the input.\\

% TODO sposta sotto in architecture 
%Now the agent starts from a fixed position specified with local coordinates of Unreal Engine or with GPS coordinates (inside \textit{\acrlong{g3dl}}). Then it takes off moving along the Z-axis until it reaches the height of the first point and continues its navigation following the trajectory obtained from the previous phase. \\
%When the agent collides with any mesh in the 3D environment the environment is reset and the agent obtains a penalty. The same applies when the agent collides with one or more points belonging to the other trajectories. 

\section{Real World Testing Layer} \label{pl4}
\textbf{\acrlong{rwtl}} is based on the approach used in \cite{vaeRL} and was designed to be a safe and controlled testing environment but as close as possible to the environment used for virtual training. 

For obvious reasons of feasibility this layer could not be implemented nor be tested in \acrshort{malrl} but will remain a conceptual design useful for possible future work and it is needed to understand the possibilities offered by \acrshort{malrl} for the future of UAV autonomous systems.

In our case, the perfect location to conduct real tests would be the portion of Barcelona city, on which the reported experiments were conducted, or a real model of it, accurate enough to minimize the mismatch between \acrshort{g3dl} layer building positions and real ones.

Doing this would allow keeping unchanged the trajectories, now corrected with the local "patch" system and expressed as a collection of GPS waypoints so ready to be followed by real UAVs for their missions. As the aforementioned paper, this can be also a very interesting way to test how accurate the trajectory generation and patch methods were, introducing new challenges as wind forces, noises, battery consumption, visual obstruction for UAV cameras, and many others.

Another possible feature could involve the introduction of real gates similar to the ones defined in layers \acrshort{g3dl} and \acrshort{s3dl}, that without modifying much the training architecture used for RL patching, could let to the dynamic activation of image-based autonomous navigation in problematic situations. Think for example of a situation in which a new geofence or no-fly zone is enforced while UAVs are flying on their pre-computed trajectories. UAVs affected by this change could be redirected to other trajectories using different gates (maybe one color per direction on the axis for the XY plane, or different shapes).

%\todo[inline]{Insert rendering of future implementation }



\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/others/Screenshot (60).png}
	
		\caption{
		Side and top view of: a) Circuit track, and b) S-shape track. Images taken from \cite{vaeRL}.
	}
	\label{fig: vaeIdea}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/others/render (2).jpg}
\caption{
		Rendering of a possible future real-world application inspired by \cite{vaeRL}, where gates of different shape help guidining the drones correcting direction.
	}
	\label{fig: vaeC}
\end{figure}

\chapter{MALRL Framework}

Inspired by the related work reported in Chapter 2 and considering the endless possibilities offered by the ever-growing research in the field of Reinforcement Learning and in particular in solutions to multi-agents pathfinding problems, the desire to explore those solutions in a 2D environment has taken hold.

Moreover, the simplicity to build complex and highly customizable 3D environments in Unreal Engine together with AirSim simplicity and accuracy in simulating UAVs missions guided the work in the direction of a framework able to merge these two different levels of abstraction.

Another opportunity has been offered by the possibility to take advantage of cities with a grid plan structure, suitable for a 2D grid plane maze-like representation.
Combining this last point with the presence of downloadable open geographic data, including 3D buildings and streets, the whole thing moved in the view of a more and more realistic test-bed.

In light of these considerations a framework composed of multiple interconnected abstraction layers, each one characterized by its problem definition, approximation constraints, and solutions, seemed to be an interesting and challenging design.

In the following section, an in-depth explanation of the structure of \acrshort{malrl} framework as a whole is presented.

\section{Architecture}
\acrshort{malrl} framework is composed of exactly four layers: \acrshort{gl}, \acrshort{s3dl}, \acrshort{g3dl}, and \acrshort{rwtl}, they have been already   introduced in Chapter \ref{chap:pdc} where each layer has been described in terms of design choices and arisen challenges. The focus of this section will be on the architecture design to provide a general overview of \acrshort{malrl} while specifying the interconnections between the layers, essential to fulfilling homogeneously the proposed objectives, achieving scalability and modularity.  


\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/arch/Architecture7.png}
	\caption{
		Architectural scheme of \protect\acrshort{malrl} framework, showing the four layers of  abstraction.\protect\footnotemark
	}
	\label{fig: malrl architecture}
\end{figure}
\footnotetext{*\acrshort{rwtl} is conceptually defined, its implementation is planned for the future}

Figure \ref{fig: malrl architecture} presents the general architecture of \acrshort{malrl} framework as a block diagram. Different layers are represented as dashed blocks, while the order of execution follows the numeration associated to layers. Starting from \acrshort{gl}, $n$ starting cells ($s_{0,1},...,s_{0,n}$) and goal cells ($g_0,...,g_n$) are passed as input to blocks representing $Q$-learning training, together with the previous $i-1$ generated trajectories ($T_1,T_2,...,T_{i-1}$) for the $i$-th block, used to reduce the generation of overlapping trajectories.

Now we are ready to move to \acrshort{s3dl} where altidue assignment procedure resides. Here the set of 2D trajectories from \acrshort{gl} ($T_1,...,T_n$) are transformed into a new set of 3D trajectories ($T'_1,..,T'_m$, $n\ge m=\text{num. of altitude assigned trajectories}$). Then AirSim's role comes into play since the choice of a starting point among the ones passed in input to \acrshort{gl} ($s_{0,c} \in \{s_{0,1},...,s_{0,n}\},s_{0,c} \in T_c $) will make AirSim's drone follow it. If the drone encounters a collision a "patch" procedure starts correcting problematic parts of the trajectory. At the end we will have a corrected trajectory $T''_c$ ready to be passed to the next layer.

The design of \acrshort{g3dl} is conceptually similar to the previous, but now we need to pass to AirSim a starting point defined as latitude and longitude to place the UAV, then we let it follow $T''_c$ and correct eventual new collisions related to the introduction of new realistic elements or the absence of abstracted objects from \acrshort{gl} environment.  

Finally, we will have in \acrshort{rwtl} a continuous trajectory composed of GPS waypoints that a real UAV could follow, in the same geographic locations chosen from training. 
% Qui parlo della possiblle correzione a run time nella realta basata sul sistema a gate simile a quella presente nel lavoro della Carneige Mellon?



\section{Separation-aware Trajectories}

As already discussed, the analysis of trajectories for estimating collision risks was one of the objectives of this thesis and required considerable effort while designing and developing \acrshort{malrl}. Specific methods to generate trajectories that take into account separation guidelines have been designed, implemented for \acrlong{s3dl}, and used also for the next layers. 
Several versions of the algorithm were created to accommodate possible needs that could depend on the characteristics of the application scenario such as the possibility to change altitude during the mission to avoid other UAVs or be constrained at a constant altitude similar to what vertical separation (sec. \ref{verSep}) does in civil aeronautics.

%\LI{Parliamo di conflitti e non di collisioni}

Two different types of conflicts can be identified: 
1) conflicts with dynamic, moving objects, in this case, other UAVs represented as trajectories, and 2) conflicts with static obstacles, in this case, composed of 3D Buildings and ground. 

As for increasing the granularity and general precision of a 2D map, the introduction of additional realistic elements in the map at \acrshort{gl} can be considered.
For example, other moving or static elements, such as electric wires of powerlines, streetlights, neon signs, no-fly zones, geofences, could be added to the environment to model specific requirements of an operational scenario.

For this thesis, static objects are considered avoided in \acrshort{gl} after having mapped them inside the aforementioned grid plane (each state has a set of acceptable actions associated with, crossing a wall is not permitted). Potential conflicts between UAVs instead should be considered also in \acrlong{s3dl}, since reinforcement learning should have discouraged overlapping paths but the absence of an optimality demonstration, in some cases impossible to reach due to the proximity of initial positions or goals, does not offer guarantees in this sense. 

Regarding the conflicts with dynamic objects, the idea is to assign an altitude to waypoints of UAVs trajectories such that already explored paths are avoided inside the city, represented by the trajectories generated in \acrshort{gl}, this is done by storing them as KD trees, details about this structure are specified in the next section. 

\section{Altitude Assignment Algorithms}
The assignment of the correct altitude to each UAV mission plan, until now simplified in the concept of trajectory as an ordered set of points, is considered crucial to decrease the risk of collision. For this thesis and also in the context of the BUBBLES project some limited tolerance is granted for the proximity of trajectory points, considering the implementation of other layers of safety and algorithms in charge of enforcing the minimum possible risk in the future.
A pseudo-code of the first designed algorithm is presented below. 

\newpage

\begin{algorithm}[H]
\caption{\textbf{Altitude assignment for empty space}}
\label{alg:altAss1}
\begin{algorithmic}[1]
\Require
 	\State $T^{l1}$ is the set of $n$ trajectories from \acrshort{gl}
 	\State $o_a$ is the vertical separation value (offset in altitude)   
 	\State $m_a$ is the minimum possible altitude
 	\State $M_a$ is the maximum possible altitude
 	\State $m_{sp}$ is minimum number of safe-points
 	\State $r$ is the radius to find how many points from other trajectories   
\Procedure{AltitudeSchedulerV1}{}
\State trees $\gets \text{Build array of kd trees from } T^{l1}$
\State drones $\gets$ Get index i for each $T_i \in T^{l1}$
\State $T^{l2} \gets$ Init empty set of trajectories $ T_0^{l2},...,T_n^{l2} $
\State    collidingTrajs $ \gets$ Init empty dictionary

\For{$T_i$ in drones}
\For{point in $T_i$}
\State nSafePoints $\gets$ 0
\State nCollisions $\gets$ 0
\For{tree in trees}
\If{tree belongs to $T_i$} \Comment{It is the tree of current trajectory}
\State go to next tree
\EndIf
\State nCollisions $\gets$ Compute the number of collision in radius $r$ from point using tree
\If{nCollisions > 0}  
\State                  collidingTrajs[i].append(indexOf(tree)) if not present
\EndIf
\EndFor
\If{nCollisions = 0}
\State            nSafePoints $\gets$ nSafePoints$\gets$ nSafePoints$+ 1$
\EndIf
\If{nSafePoints $\ge m_{sp}$ }
\State            collidingTrajs[i] $\gets$ $\varnothing$
\EndIf
\If{ i not in collidingTrajs or collidingTrajs[i]=$\varnothing$}
\State            newAltitude $\gets M_a$
\Else
\State            priorities $\gets$ i $\bigcup \ $ collidingTrajs[i]
\State            priorities.sort()
\State            offset $\gets$ priorities.index(i)
\State            newAltitude $\gets M_a - offset \cdot o_a $
\If{newAltitude < $m_a$}
\State
\Return $\varnothing$               
\Comment{Impossible to allocate due to $[m_a,M_a]$ limits}
\EndIf
\EndIf
\State point.z $\gets$ newAltitude
\State      $T_i^{l2}$.append(point)
\EndFor
\EndFor
\State
\Return $T^{l2}$

\EndProcedure
\end{algorithmic}
\label{}
\end{algorithm}

\newpage

This algorithm was designed for a space situation that consists of a case in which no UAVs have already be assigned to a trajectory, so no trajectory $T_i$ in input has already an associated altitude. The basic idea behind the algorithm is to give a priority to trajectories at the start position of the array in input, giving them the highest possible altitude but decreasing it by a multiple of $o_a$ when a trajectory with enough number of colliding points is found and has a greater priority than the current.

Later a second version of this algorithm was introduced to add as a feature the possibility to assign a correct altitude to trajectories of \acrshort{gl} ($T^{l1}$) given the presence of trajectories that cannot be moved since their altitude value has already been assigned ($T_{assigned}^{l2}$). In this sense, the new space from empty is now called in general "busy". Together with the introduction of this feature new aspects have been taken into account leading to the introduction of new parameters.

\newpage 

\begin{algorithm}[H]
\caption{\textbf{Altitude assignment for busy space}}
\label{alg:altAss2}
\begin{algorithmic}[1]
\Require
 	\State $T^{l1}$ is the set of $n$ trajectories from \acrshort{gl}
  	\State  $T_{assigned}^{l2}$ is the set of pre-existing 3D trajectories
  	\State $M_{points}$ is the maximum number of allowed colliding points before considering a trajectory colliding
 	\State $o_a$ is the vertical separation value (offset in altitude)
 	\State $m_a$ is the minimum possible altitude
 	\State $M_a$ is the maximum possible altitude
 	\State $m_{sp}$ is minimum number of safe-points
 	\State $r$ is the radius to find how many points from other trajectories   
\Procedure{AltitudeSchedulerV2}{}


\For{ $T_i \in T_{assigned}^{l2}$ }
\State  k $\gets$ getAltitudeOf($T_i$)
\State v = buildKDTree($T_i$) \Comment{Create 2D Tree with points from $T_i$}
\State staticTrees.insert(k,v)
\EndFor
    

\For{$T_i \in T^{l1}$} 
\State  k $\gets M_a$ 
\State v = buildKDTree($T_i$)
\State mobileTrees.insert(k,v)
\EndFor
      
\State    proposedAltitude $\gets$ Init empty dictionary
\For{ $T_i \in T^{l1}  $}
\State        proposedAltitude[i]  $\gets M_a$
\EndFor
\algstore{myalg}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\begin{algorithmic}[1]
\algrestore{myalg}
\For{$ T_i \in T^{l1} $}
\State        assigned $\gets$ False
\While{assigned $\neq$ True}
\For{$ point \in T_i$}
\If{$proposedAltitude[i] \in staticTrees.keys()$}
\For{ tree in staticTrees[proposedAltitude[i]]}
\State nCollisions $\gets$ nCollisions+tree.findCollisionsFrom(point,$r$)
\If{ nCollisions > 0 }
\State                            nProblematic $\gets$ nProblematic + 1
\State \textbf{break}
\EndIf
\EndFor
\EndIf

\If{proposedAltitude[i] in mobileTrees.keys()}
\For{ tree in mobileTrees[proposedAltitude[i]]}
\State \textbf{continue} 
\If {$i$ = indexOf(tree)}
\Comment{Tree associated with $T_i$}
\EndIf
\State nCollisions $\gets$ nCollisions+tree.findCollisionsFrom(point,$r$)
\If{ nCollisions > 0}
\State nProblematic $\gets$ nProblematic + 1
\State \textbf{break}
\EndIf \EndFor \EndIf
\If{nProblematic $\ge M_{points}$}   
\State mobileTrees[proposedAltitude[i]].remove(getTreeByIndex(mobileTrees,i)) 
\State proposedAltitude[i] $\gets$  proposedAltitude[i] - $o_a$
\If{proposedAltitude[i] < $m_a$} 
\State
\Return $\varnothing$
\EndIf
\State                    mobileTrees.insert(proposedAltitude[i],getTreeByIndex(mobileTrees,i))
\State \textbf{break}
\EndIf
\EndFor

\If{nProblematic < $M_{points}$}
\State assigned $\gets$ True
\EndIf
\EndWhile
\EndFor

\State    $T^{l2} \gets$  $\varnothing$
\For{ $i \in $ proposedAltitude.keys()}
\State newAltitude $\gets$ proposedAltitude[i]
$newT_i$ = $\varnothing$
\For{ point in $T_i$ }
\State point.z $\gets$ newAltitude
\State $newT_i$.append(point) 
\EndFor
\State $T^{l2}$.append($newT_i$)
\EndFor
\Return $T^{l2}$

\EndProcedure
\end{algorithmic}
\end{algorithm}

This was the pseudocode of the algorithm used to conduct tests in the results chapter. 
However other algorithms extending Alf. \ref{alg:altAss2} have been implemented to prepare \acrshort{malrl} for eventual other types of air space constraints regarding how to manage the collision risk. 

In all tests the problem of altitude assignment was formulated to be as a single altitude value per all points composing the trajectory, not allowing descents.
The same altitude value (equal to the maximum height at the beginning) is shared by not conflicting trajectories on the same XY plane, decreasing the altitude of a separation value each time a point is distant less than the specified radius from another point of another trajectory, we call this last situation as a "conflict". A variable called tolerance ($M_{tolerance}$) has been also introduced to define a different altitude assignment policy in which each trajectory is considered conflicting only after $M_{points}$ of its points are conflicting, where $M_{points}= f(T_i,M_{tolerance})$ and 

\[   
M_{points} = f(T_i,M_{tolerance}) =
\begin{cases}
1 &\quad\text{if }  M_{tolerance} = 0 \\
 \lfloor \vert T_i^{l1} \vert \rfloor \cdot M_{tolerance} &\quad\text{if }  M_{tolerance} \ne 0 

\end{cases}
\]
\\
When $M_{points}=1$ we fall in the previous case.

A second mode allows descents and lets a trajectory be kept at the assigned height, i.e. maximum at the beginning and decreased of a separation value after a conflict, until a new conflict point is found then it is decreased again. 

A third mode tries to keep the trajectory at maximum altitude as much as possible and decrease its value for each trajectory point, only if it is a conflicting point or one of the $m$ (constant to be set) consecutive points, considering this subset of the trajectory as a de-conflicting space. 


\section{Scalability}

Scalability is one of the main objectives of this project and it is intended to in terms of modeled UAVs, so number of trajectories generated. Regarding \acrshort{gl}, dimensions are easily expandable considering the possibility to change the scale associated with each 2D block composing the grid or to simply change map dimensions as the amount of rendered blocks. In this sense we can talk also of a scalability in terms of space dimensions.

\acrlong{s3dl} has been designed to be smaller and simpler than the next Layer to ease computation and the work of the operator that is modeling the environment directly inside Unreal Engine 3D editor. However, this layer, as well as the next one (\acrlong{g3dl}), is heavily extensible considering that graphics elements, with high level of detail, could be introduced.

The chapter 7.1 will show results that confirm these principles and gives room for discussion about limitations and possible improvements.



\chapter{Implementation Details}

\section{General Usage}
Each layer (except for \acrlong{rwtl}, cf \ref{pl4}) has a specific homonym Python script associated with it, ready to be launched. Scripts must be executed one by one following the numerical order so starting from the first layer then second, third and fourth. Any other modality of execution is not currently supported.

The passage of data is done using CSV files to store and then retrieve generated trajectories $T^{l1},T^{l2},T^{l3}$. This lets us analyze them easily, as it has been done to pass trajectories to the external risk model.  

\section{2D Reinforcement Learning}

As specified in the previous chapter, \acrshort{gl}'s problem is formulated as a Reinforcement Learning problem. In particular, the $Q$-learning algorithm has been implemented from scratch and tested also in its K-steps version. The maze instead (\acrshort{gl}'s environment) follows OpenAI Gym format so it is compatible with other environments sharing a similar structure (same action space and state space) and with other RL algorithms that follow Gym APIs.

The \textit{\acrshort{gl}} of the Framework is composed of a maze environment customizable and compatible with \textit{OpenAI Gym} toolkit enabling the possibility to apply other algorithms and test them against $Q$-learning.

This leaves room for many different RL algorithms to be used for training however the objectives of this thesis go beyond the optimization of RL methods.
%\LI{aggiungi un commento che avresti potuto usare qualsiasi altro algoritmo di RL, ma che non ci interessa in questa tesi l'ottimizzazione sui metodi di RL... }

\section{Maze Environment}
Our maze environment in \acrshort{gl} is inspired by \cite{mazeGit} where a simple classic maze with random walls is implemented. The clean and easily extendable code offered the possibility to use it as a starting point in coding our custom 2D environment representing a city portion in a grid plane. Some elements have been exploited as the bitmap array used to encode all possible actions inside the grid plane. In this way escaping from boundaries or entering the cell of obstacles cannot be chosen as possible actions. 

Some elements of the original maze were discarded while others have been built from scratch. For example, the original maze contained portals, cells in which the agent can enter to be teleported into another cell, without taking actions needed to reach that cell. 

Talking about the main new elements introduced in the code, we have the introduction of multiple goals mode, where each of them needs to be collected to complete an episode; the covering mode, in which the agent completes the episode only when all free cells of the board (the ones that represent neither obstacles neither goals nor starting points) have been visited; the history of precedent actions, to save previous trajectories to discourage the generation of overlapping paths.

These new elements and the general new design of the maze as a symmetric grid plane city with a square group of $6\times 6$ blocks were accompanied by some improvements in the part of the code related to the rendering of the maze, disabled by default but it can be enabled through the relative CLI parameter. 



\section{AirSimGeo}
To build a geo-referenced world the following steps were used:

first, the area of interest (min-max latitude,min-max longitude) was identified from the map available on the OpenStreetMap website and downloaded as an OSM file.
Then opening Blender open-source 3D editor it was installed and used \textit{blender-osm}. It is an open-source plugin for Blender, easy to install, and ready to be used that offers the possibility to download, load, manipulate OSM maps to transform them into 3D maps and texture overlay.
So from the downloaded OSM file after some adjustment, a 3D scene was auto-generated from blender-osm in Blender, then saved as FBX and PNG files.

These two files were imported inside Unreal Engine, applying auto-detection of collision meshes. Simple collisions were used instead of complex even if they are known to be less optimized, they did not affect significantly the computation.

At this point, AirSim was ready to be used inside the georeferenced environment where new API calls were needed to extend the UAV client with commands able to move the vehicle towards specified latitude and longitude coordinates. This was possible thanks to \textit{AirSimGeo} repository that extends AirSim’s “MultirotorClient” with new functions:

\begin{itemize}
	\item getGpsLocation: to retrieve GPS position of UAV transforming from local coordinates to GPS georeferenced ones
	\item moveToPositionAsyncGeo: to move asynchronously UAV towards a specified GPS position.
	\item moveOnPathAsyncGeo: to move asynchronously UAV towards specified GPS waypoints.
\end{itemize}

Blender and Unreal Engine are both defined in a cartesian coordinates system (x,y,z) and therefore a projection must be made to translate passing from NED (North East Down) coordinates to them and finally to GPS coordinates. 

\chapter{Experimental Results} \label{cres}
This chapter is dedicated to the empirical results obtained experimenting with some specific aspects of \acrshort{malrl} framework. Starting from \acrshort{gl} results related to Reinforcement Learning then the experiments focus on altitude assignment in \acrlong{s3dl}'s algorithm, exploring the existing trade-off between scalability, in terms of the number of considered trajectories, and time efficiency.

\section{Grid Layer with Horizontal Separation}
The notation used in the following chapter is described in Section \ref{pl1}.

\subsection{Evaluation Criteria}
%\LI{Togliere parte random agent e scrivere che un random agent non ce la fa}
Reinforcement Learning scalability in terms of the number of generated trajectories is evaluated by comparing the results obtained using $Q$-learning with the ones obtained using K-steps $Q$-learning algorithms. Plots show the reward function behavior when increasing the number of episodes and the number of steps required in each episode while tables compare the different total elapsed time for training when using different algorithms and <$S_e,G_e$> couples.

Reward plot through the different experiments will highlight how many episodes are required to complete the goal with low penalization, meaning in our case fast navigation towards the goal, while training to avoid other trajectories, standing still in a cell or exploring moving away from the target.
Steps per episode instead can provide a measure of the time required from the agent to reach the goal in the specific step.

This information together with a measure of computational time required to train the agent, considering the number of different initial and final positions tested can be sufficient to draw conclusions about the experiments conducted in \acrshort{gl}.


\subsection{Experiments}
Three main experiments have been conducted, each one has been composed of learning and testing using in the order: a random policy maximizing the reward obtained, standard $Q$-learning algorithm, and K-steps version of $Q$-learning.

To compare the different learning algorithms,while varying the number of trajectories, each experiment has been conducted keeping unchanged both the sequence of initial positions $S_e=\{s_{0,0},s_{0,1},...,s_{0,n_e}\}$, where $s_{0,i}$ is the initial position of $i$-th trajectory in the experiment $e$, $n_e$ is the number of generated trajectories for experiment $e$, and the sequence of goals (desired final positions) $G_e=\{g0,g1,...,g_{n_e}\}$. It should be highlighted again that for our experiments each agent should collect exactly one single goal to consider concluded the training episode, but a mode with multiple goals per episode is implemented, although not tested. Another important observation is relative to the penalization mechanism (by negative reward) of overlapping trajectories, already been defined in the chapter \ref{glpl}, where the buffer of previous trajectories is erased when it holds ten previous trajectories.

To acquire results covering different cases, each triplet of experiments has been repeated three times choosing each time a different couple of sequences <$S_e,G_e$> for every triplet.

Each experiment shared the following training parameters:
\begin{itemize}
    \item \textbf{Seed}: $111$
    \item \textbf{Num. of episodes}: $200$
    \item \textbf{Value of k (K-steps $Q$-Learning only)}: $20$
    \item \textbf{Max. num. of steps per episode}: $10000$
    \item \textbf{Decay factor (to decrease explore and learning rate over time)}: $\delta = 185$ 
    \item \textbf{Min. Learning rate}: $\alpha_m=  0.2$ 
    \item \textbf{Learning rate}: $\alpha = \max \{  \alpha_m, \min \{ 0.8, 1.0 - 
            \log_{10}[\frac{(episodeNum +1)}{\delta} ] \}\} $
    \item \textbf{Min. explore rate}: $\epsilon_m = 0.001$ 
    \item \textbf{Explore rate}: $\epsilon = \max \{  \epsilon_{m}, \min \{ 0.8, 1.0 - \log_{10}[\frac{(episodeNum +1)}{\delta} ] \} \} $
    \item \textbf{Number of trajectories generated in experiment $e$:}

\[   
n_e = f(e) =
\begin{cases}
10 &\quad\text{if }  e = 1 \\
20 &\quad\text{if }  e = 2 \\  
50 &\quad\text{if }  e = 3 \\ 

\end{cases}
\]
\\
\end{itemize}

Plotting the results obtained from a random agent will be useless considering that in all cases steps required to reach the goal were above the maximum allowed number of steps per episode, thus this results are not reported and $Q$-learning is considered successful in generating a trajectory better than random.

The following are tables and plots obtained from training experiments with the above parameters, to compare $Q$-learning and K-steps $Q$-learning plots on scalability. Notice that the result of each trajectory is averaged with the other to make the comparison easier. On the left column, you will find results obtained from $Q$-learning while on the right the ones obtained from K-steps $Q$-learning (K=20).
We start with the plot regarding the trend of the reward function, where the same couple of sequences <$S_e,G_e$> have the same color.



\newpage
%======================= RW e=1 ======================


\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{figures/glPlots/Q1FRW.png}
	\caption{}
	\label{fig: rwe11}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/glPlots/NS1FRW.png}
  \caption{}
  \label{fig:rwe12}
\end{subfigure}
\caption{Rewards per episode in <$S_1,G_1$>, $Q$-learning vs K-steps $Q$-learning ($K=20$) }
\label{fig:rwe1}
\end{figure}
%======================= RW e=2 ======================

\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{figures/glPlots/Q2FRW.png}
	\caption{}
	\label{fig: rwe21}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/glPlots/NS2FRW.png}
  \caption{}
  \label{fig:rwe22}
\end{subfigure}
\caption{Rewards per episode in <$S_2,G_2$>, $Q$-learning vs K-steps $Q$-learning ($K=20$) }
\label{fig:rwe2}
\end{figure}
%
%======================= RW e=3 ======================
\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{figures/glPlots/Q3FRW.png}
	\caption{}
	\label{fig: rwe31}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/glPlots/NS3FRW.png}
  \caption{}
  \label{fig:rwe32}
\end{subfigure}
\caption{Rewards per episode in <$S_3,G_3$>, $Q$-learning vs K-steps $Q$-learning ($K=20$)) }
\label{fig:rwe3}
\end{figure}



%======================= ST e=1 ======================


\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{figures/glPlots/Q1FS.png}
	\caption{}
	\label{fig: ste11}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/glPlots/NS1FS.png}
  \caption{}
  \label{fig:ste12}
\end{subfigure}
%\vspace*{-10mm}
\caption{Number of steps per episode in <$S_1,G_1$>, $Q$-learning vs K-steps $Q$-l. ($K=20$) }
\label{fig:ste1}
\end{figure}
%======================= ST e=2 ======================

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{figures/glPlots/Q2FS.png}
	\caption{}
	\label{fig: ste21}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/glPlots/NS2FS.png}
  \caption{}
  \label{fig:ste22}
\end{subfigure}
\caption{Number of steps per episode in <$S_2,G_2$>, $Q$-learning vs K-steps $Q$-l. ($K=20$) }
\label{fig:ste2}
\end{figure}
%
%======================= ST e=3 ======================
\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{figures/glPlots/Q3FS.png}
	\caption{}
	\label{fig: ste31}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/glPlots/NS3FS.png}
  \caption{}
  \label{fig: ste32}
\end{subfigure}
\caption{Number of Steps per episode in <$S_3,G_3$>, $Q$-learning vs K-steps $Q$-l. ($K=20$)) }
\label{fig:ste3}
\end{figure}


\newpage

This table shows times for training when using the parameters specified above:

\begin{table}[H]
\begin{center}
\begin{tabular}{ |p{2cm}|p{2cm}|p{4cm}||p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{\textbf{Time comparison}} \\
 \hline
 \textbf{<}$S_e,G_e$\textbf{>}   & Num. of generated trajectories  & \textbf{Algorithm} & \textbf{Avg. Time Required (s)}  \\
 \hline
  <$S_1,G_1$> & 10 & $Q$-Learning & 216.276  \\
 \hline
  <$S_1,G_1$> & 10 & K-steps $Q$-Learning & 519.129 \\
 \hline
 \hline
  <$S_2,G_2$> & 20 & $Q$-Learning & 662.23  \\
\hline
  <$S_2,G_2$> & 20 &K-steps $Q$-Learning & 1156.966  \\
     \hline
        \hline
  <$S_3,G_3$> & 50 &$Q$-Learning & 1356.628  \\
\hline
  <$S_3,G_3$> & 50 & K-steps $Q$-Learning &  3721.250  \\
     \hline
 \hline

\end{tabular}
\caption[$Q$-learning  vs K-steps $Q$-learning  comparison]{$Q$-learning  vs K-steps $Q$-learning  comparison}
\label{tbl:table1}
\end{center}
\end{table}
  
\subsection{Discussion}
As expected K-steps $Q$-learning shows a slower training of the policy due to the introduction of k-steps memorization that requires updating the Q-table K more times compared to the corresponding standard $Q$-learning. The amount of time required will increase when increasing the value of k, standing as a value that needs to be specifically balanced depending on the structure of the environment in the analysis. Indeed if k would be equal to the number of cells connecting the initial position to the goal, Q-table would be updated for all the states involved as soon as a goal is collected by the agent, however, in a large maze structure, this will not be feasible. These results show the scalability of the system and its limitation for a predefined portion of space. It needs to be considered the over-computation related to the presence of previous trajectories, having at most one trajectory fitting in the streets of the grid, so techniques of reward shaping or larger streets could help in this sense, however it should be highlighted that our solution is required off-line.


\section{Simplified 3D Layer with Vertical Separation}
For this section, the focus was posed on the scalability and time efficiency of the algorithm (Alg. \ref{alg:altAss2}) under different conditions relative to the vertical separation problem.

\subsection{Evaluation Criteria}
The following section will be devoted to showing results from experiments conducted on the algorithm of altitude assignment in busy space (Alg. \ref{alg:altAss2}) that used to give vertical separation between trajectories.
The analysis will focus on comparison about risk model results when changing training parameters such as considered radius for collision and number of points needed to shift a trajectory to a new altitude, along with the total number of considered trajectories. As an evaluation method, it was used a visual analysis of the obtained trajectories along with a simple count of the number of trajectories assigned compared to the number of trajectories that were refused, when increasing the number of 2D trajectories given in input.  


\subsection{Experiments}

Using a random generator, to reduce times reported in table \ref{tbl:table1}, from the previous section, which are related to the choice of parameters and the partiality of code optimization, $n \in {100,500,1000}$ trajectories are analyzed, they have the same characteristics of trajectories obtained from \acrshort{gl}, where each point belongs to $\R^2$ and their structure emulates the movement of an agent around buildings. So from a point $s_t \in T_i^{l1}$ the next one $s_{t+1} \in T_i{l1}$ with $i=0,...,n$ is generated to have a distance $d(s_t,s_{t+1})=c$, where $c=120$ is a constant chosen to be similar to the one used to represent the edge of buildings in \acrshort{gl}. Then these trajectories are processed by a script implementing (Alg. \ref{alg:altAss2}) to obtain $T_i^{l2} = \{s_0',...,s_T'|s_t'\in \R^3, s_t'=(s_t,a_t),t \in [0,T] \}$.

For these tests the possibility to pass as read-only input some already assigned trajectories is exploited, this is done to simulate an eventual application of these methods in a real context where new trajectories are dynamically assigned although some drones are already flying in their assigned trajectories. The generation works as follows:
group of ten 2D trajectories is passed to the algorithm as new trajectories to be assigned, when this happens they are saved and passed as input in the next iteration as read-only 3D trajectories together with the new group of ten 2D trajectories to be allocated.    

It is reminded that the symbol $M_{points}$ is used to represent the threshold value for the number of colliding points of a trajectory $T_i^{l1}$, after which the trajectory needs to be shifted to a new altitude. 
Since the length of each trajectory, in terms of the number of points composing it, is unpredictable, the threshold value ($M_{points}$) directly depends on the parameter tolerance ($M_{tolerance}$) by this simple equation:
\[   
M_{points} = 
\begin{cases}
1 &\quad\text{if }  M_{tolerance} = 0 \\
 \lfloor \vert T_i^{l1} \vert \rfloor \cdot M_{tolerance} &\quad\text{if }  M_{tolerance} \ne 0 

\end{cases}
\]
\\

Should be considered that depending on the parameters passed, it is possible to have a trajectory that cannot be allocated due to the full coverage of all the slots for allowed altitude.
Six different tests (each test has been repeated ten times to acquire consistency in the results, changing the seed and then averaging) have been conducted, all sharing these parameters:


 \begin{itemize}

\item\textbf{Radius} : 200 m  
 
\item \textbf{Min x coordinate }: 0 m 
 
\item \textbf{Max x coordinate }: 1000 m  
 
\item \textbf{Min y coordinate }:  0 m
 
\item \textbf{Max y coordinate }: 0 m
 
\item \textbf{Min altitude coordinate (z)}: 50 m 
 
\item \textbf{Max altitude coordinate (z)}: 300 m
 
\item \textbf{Step ($c$)}: 120 m
 
\item \textbf{Vertical separation offset}: 200 m
 
 
 \end{itemize}


The following table contains the results obtained in the six tests:
%\LI{inserire che sono in 2D}

\begin{table}[H]
\begin{center}
\begin{tabular}{ |p{3.5cm}|p{2cm}||p{2cm}|p{2cm}|p{2cm}|  }
 \hline
 \multicolumn{5}{|c|}{\textbf{Parameter comparison}} \\
 \hline
 \textbf{Threshold} ($M_{points}$)     & \textbf{Num. of generated 2D trajectories ($n$)} & \textbf{Avg. Time Required (s)} & \textbf{Num. of assigned trajectories} & \textbf{Num. of refused trajectories}\\
 \hline
  $M_{points}=1$ & 100 &  0.856 & 95 & 5\\
  
 \hline
  $M_{points}=20$ & 100 &  1.25 & 96 & 4\\
  
 \hline
 \hline
  $M_{points}=1$ & 500  & 5.674 & 89 & 411\\
  
 \hline
  $M_{points}=20$ & 500  & 7.532 & 178 & 322\\
 \hline
 \hline
  $M_{points}=1$ & 1000 &  15.931 &  72& 928\\
  
 \hline
  $M_{points}=20$ & 1000 & 19.756 & 122& 878\\

 \hline
\end{tabular}
\caption[Assigned trajectories for different tolerance and number of drones ]{Assigned trajectories for different tolerance and number of drones }
\label{tbl:table2}
\end{center}
\end{table}
The integration of results reported in the table with visual analysis from the generated 3D plot can help to draw our conclusions. Hence what follows are the most significant plots (remind that ten samples have been generated per test) from the aforementioned six tests.


\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=2.0\textwidth]{figures/altPlots/test13d.png}}%
	
	\caption{ 3D plot of test on altitude assignment ($n=100$, $M_{tolerance}=0 \% $)}
	\label{fig: e211}
\end{figure}

\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=2.0\textwidth]{figures/altPlots/test23d.png}}%
	
	\caption{ 3D plot of test on altitude assignment ($n=100$, $M_{tolerance}=0.05 \% $)}
	\label{fig: e212}
\end{figure}

\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=2.0\textwidth]{figures/altPlots/test33d.png}}%
	
	\caption{ 3D plot of test on altitude assignment ($n=500$, $M_{tolerance}=0 \% $)}
	\label{fig: e221}
\end{figure}

\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=2.0\textwidth]{figures/altPlots/test43d.png}}%
	
	\caption{ 3D plot of test on altitude assignment ($n=500$, $M_{tolerance}=0.05 \% $)}
	\label{fig: e222}
\end{figure}

These were the plots to show what kind of result altitude asigner can obtain. It is suggested to integrate this plot with the ones present in appendix,  since an interactive visualization would be required due to the high number of 3D trajectories involved. For this reasons fifth and six tests relative to $n=1000$ were not included in this section, however, they can be found in the appendix. 


\subsection{Discussion}
Altitude assignment was a crucial objective this thesis wanted to achieve in the contest of collision risk minimization. Results showed how k-dimensional trees could be an efficient static data structure to be scanned and used for solving this problem. Many different parameters come into play when doing a deep analysis of the result obtained in terms of collision risk minimization, but the results showed the capacity to scale in terms of the number of trajectories generated (considered to be the number of UAVs in the system).

%\section{Simplified 3D Layer with Horizontal and Vertical Separation}
%\section{Georeferenced 3D Layer Example of Generated Trajectory}

\chapter{Conclusions and Future Work}
\acrlong{malrl} framework could be an example of how it is possible to design a framework, able to generate scalable, credible trajectories, reducing the existing gap between high-level abstracted RL environments and real-world deployment via realistic modeling and open data georeferencing. In this sense, \acrshort{malrl} represents a first step in the process of creating a Python-based, multi-platform, modular, and scalable framework. This study exploits the symmetry present in a specific Barcelona city's district, to build a simple bi-dimensional grid model of it. A custom version of $Q$-learning is implemented and tested to iteratively generate bi-dimensional trajectories, partially emulating an exploratory activity. Generated trajectories are then passed as input to an algorithm that takes care of assigning a correct value of the altitude coordinate to each trajectory, depending on the parameters defined for the scenario under study.

Experiments on these methods were conducted and evaluated, to measure RL training (\acrshort{gl})'s and altitude assigner's (\acrshort{s3dl}) results in terms of the number of trajectories and time-efficiency. Following the results from the BUBBLES project, it is also possible to obtain a numerical measure of the specific collision risk. 

In AirSim the obtained 3D trajectories are visualized and followed by realistic agents, until collision with obstacles occurs, triggering a local trajectory patcher system. Accurate satellite open data and collision-responsive 3D rendering of buildings could help in the implementation of a real-world application, that it has not been possible to test but could be part of future work.  

 This study shows the potential of using reinforcement learning algorithms, open satellite data, 3d editors, and 3d simulators, combined with specific algorithm design and integrated into a multi-layer framework to make a scalable multi-UAV collision risk minimization test-bed. The pipeline of methodologies presented in each layer could be extended and customized depending on the environment and task needs of different scenarios.
 
 
\chapter*{Appendix}
\section{Additional Altitude Assignment Plots}


\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test1xz.png}}%
	\caption{ XZ plot of test on altitude assignment ($n=100$, $M_{tolerance}=0 \% $)}
	\label{fig: a1}
\end{figure}


\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test1yz.png}}%
	\caption{ YZ plot of test on altitude assignment ($n=100$, $M_{tolerance}=0 \% $)}
	\label{fig: a2}
\end{figure}


\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test2xz.png}}%
	\caption{ XZ plot of test on altitude assignment ($n=100$, $M_{tolerance}=0.05 \% $)}
	\label{fig: a3}
\end{figure}



\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test1xz.png}}%
	\caption{ YZ plot of test on altitude assignment ($n=100$, $M_{tolerance}=0.05 \% $)}
	\label{fig: a4}
\end{figure}


\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test1xz.png}}%
	\caption{ XZ plot of test on altitude assignment ($n=500$, $M_{tolerance}=0 \% $)}
	\label{fig: a5}
\end{figure}


\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test1xz.png}}%
	\caption{ YZ plot of test on altitude assignment ($n=500$, $M_{tolerance}=0 \% $)}
	\label{fig: a6}
\end{figure}


\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test1xz.png}}%
	\caption{ XZ plot of test on altitude assignment ($n=500$, $M_{tolerance}=0.05 \% $)}
	\label{fig: a7}
\end{figure}



\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test1xz.png}}%
	\caption{ YZ plot of test on altitude assignment ($n=500$, $M_{tolerance}=0.05 \% $)}
	\label{fig: a7}
\end{figure}


\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test43d.png}}%
	\caption{ 3D plot of test on altitude assignment ($n=1000$, $M_{tolerance}=0 \% $)}
	\label{fig: a7}
\end{figure}



\begin{figure}[H]
	\centering
	  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/altPlots/test53d.png}}%
	\caption{ 3D plot of test on altitude assignment ($n=1000$, $M_{tolerance}=0.05 \% $)}
	\label{fig: a7}
\end{figure}



% bibliography
\cleardoublepage
\printglossaries


\phantomsection
\bibliographystyle{sapthesis} % BibTeX style
\bibliography{bibliography} % BibTeX database without .bib extension

%\LI{Usa misc nella bibliografia per i riferimenti online, vedi esempio con BUBBLES}


\end{document}
